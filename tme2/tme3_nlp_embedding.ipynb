{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP 2 : Neural Embeddings, Text Classification, Text Generation\n",
    "\n",
    "\n",
    "To use statistical classifiers with text, it is first necessary to vectorize the text. In the first practical session we explored the **bag of word** model. \n",
    "\n",
    "Modern **state of the art** methods uses  embeddings to vectorize the text before classification in order to avoid feature engineering.\n",
    "\n",
    "## Dataset\n",
    "https://github.com/cedias/practicalNLP/tree/master/dataset\n",
    "\n",
    "## \"Modern\" NLP pipeline\n",
    "\n",
    "By opposition to the **bag of word** model, in the modern NLP pipeline everything is **embeddings**. Instead of encoding a text as a **sparse vector** of length $D$ (size of feature dictionnary) the goal is to encode the text in a meaningful dense vector of a small size $|e| <<< |D|$. \n",
    "\n",
    "\n",
    "The raw classification pipeline is then the following:\n",
    "\n",
    "```\n",
    "raw text ---|embedding table|-->  vectors --|Neural Net|--> class \n",
    "```\n",
    "\n",
    "\n",
    "### Using a  language model:\n",
    "\n",
    "How to tokenize the text and extract a feature dictionnary is still a manual task. To directly have meaningful embeddings, it is common to use a pre-trained language model such as `word2vec` which we explore in this practical.\n",
    "\n",
    "In this setting, the pipeline becomes the following:\n",
    "```\n",
    "      \n",
    "raw text ---|(pre-trained) Language Model|--> vectors --|classifier (or fine-tuning)|--> class \n",
    "```\n",
    "\n",
    "\n",
    "- #### Classic word embeddings\n",
    "\n",
    " - [Word2Vec](https://arxiv.org/abs/1301.3781)\n",
    " - [Glove](https://nlp.stanford.edu/projects/glove/)\n",
    "\n",
    "\n",
    "- #### bleeding edge language models techniques (only here for reference)\n",
    "\n",
    " - [UMLFIT](https://arxiv.org/abs/1801.06146)\n",
    " - [ELMO](https://arxiv.org/abs/1802.05365)\n",
    " - [GPT](https://blog.openai.com/language-unsupervised/)\n",
    " - [BERT](https://arxiv.org/abs/1810.04805)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Goal of this session:\n",
    "\n",
    "1. Train word embeddings on training dataset\n",
    "2. Tinker with the learnt embeddings and see learnt relations\n",
    "3. Tinker with pre-trained embeddings.\n",
    "4. Use those embeddings for classification\n",
    "5. Compare different embedding models\n",
    "6. Pytorch first look: learn to generate text.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##  Loading data (same as in nlp 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train reviews :  25000\n",
      "----> # of positive :  12500\n",
      "----> # of negative :  12500\n",
      "\n",
      "[\"The undoubted highlight of this movie is Peter O'Toole's performance. In turn wildly comical and terribly terribly tragic. Does anybody do it better than O'Toole? I don't think so. What a great face that man has!<br /><br />The story is an odd one and quite disturbing and emotionally intense in parts (especially toward the end) but it is also oddly touching and does succeed on many levels. However, I felt the film basically revolved around Peter O'Toole's luminous performance and I'm sure I wouldn't have enjoyed it even half as much if he hadn't been in it.\", 1]\n",
      "\n",
      "Number of test reviews :  25000\n",
      "----> # of positive :  12500\n",
      "----> # of negative :  12500\n",
      "\n",
      "['Although credit should have been given to Dr. Seuess for stealing the story-line of \"Horton Hatches The Egg\", this was a fine film. It touched both the emotions and the intellect. Due especially to the incredible performance of seven year old Justin Henry and a script that was sympathetic to each character (and each one\\'s predicament), the thought provoking elements linger long after the tear jerking ones are over. Overall, superior acting from a solid cast, excellent directing, and a very powerful script. The right touches of humor throughout help keep a \"heavy\" subject from becoming tedious or difficult to sit through. Lastly, this film stands the test of time and seems in no way dated, decades after it was released.', 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "#### /!\\ YOU NEED TO UNZIP dataset/json_pol.zip first /!\\\n",
    "\n",
    "\n",
    "# Loading json\n",
    "with open(\"json_pol\",encoding=\"utf-8\") as f:\n",
    "    data = f.readlines()\n",
    "    json_data = json.loads(data[0])\n",
    "    train = json_data[\"train\"]\n",
    "    test = json_data[\"test\"]\n",
    "    \n",
    "\n",
    "# Quick Check\n",
    "counter_train = Counter((x[1] for x in train))\n",
    "counter_test = Counter((x[1] for x in test))\n",
    "print(\"Number of train reviews : \", len(train))\n",
    "print(\"----> # of positive : \", counter_train[1])\n",
    "print(\"----> # of negative : \", counter_train[0])\n",
    "print(\"\")\n",
    "print(train[0])\n",
    "print(\"\")\n",
    "print(\"Number of test reviews : \",len(test))\n",
    "print(\"----> # of positive : \", counter_test[1])\n",
    "print(\"----> # of negative : \", counter_test[0])\n",
    "\n",
    "print(\"\")\n",
    "print(test[0])\n",
    "print(\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec: Quick Recap\n",
    "\n",
    "**[Word2Vec](https://arxiv.org/abs/1301.3781) is composed of two distinct language models (CBOW and SG), optimized to quickly learn word vectors**\n",
    "\n",
    "\n",
    "given a random text: `i'm taking the dog out for a walk`\n",
    "\n",
    "\n",
    "\n",
    "### (a) Continuous Bag of Word (CBOW)\n",
    "    -  predicts a word given a context\n",
    "    \n",
    "maximizing `p(dog | i'm taking the ___ out for a walk)`\n",
    "    \n",
    "### (b) Skip-Gram (SG)               \n",
    "    -  predicts a context given a word\n",
    "    \n",
    " maximizing `p(i'm taking the out for a walk | dog)`\n",
    "\n",
    "\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: train (or load) a language model (word2vec)\n",
    "\n",
    "Gensim has one of [Word2Vec](https://radimrehurek.com/gensim/models/word2vec.html) fastest implementation.\n",
    "\n",
    "\n",
    "### Train:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-17 14:08:54,863 : INFO : collecting all words and their counts\n",
      "2022-02-17 14:08:54,864 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2022-02-17 14:08:55,118 : INFO : PROGRESS: at sentence #10000, processed 2358544 words, keeping 155393 word types\n",
      "2022-02-17 14:08:55,384 : INFO : PROGRESS: at sentence #20000, processed 4675912 words, keeping 243050 word types\n",
      "2022-02-17 14:08:55,523 : INFO : collected 280617 word types from a corpus of 5844680 raw words and 25000 sentences\n",
      "2022-02-17 14:08:55,524 : INFO : Creating a fresh vocabulary\n",
      "2022-02-17 14:08:55,675 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 49345 unique words (17.584465659600095%% of original 280617, drops 231272)', 'datetime': '2022-02-17T14:08:55.674872', 'gensim': '4.1.2', 'python': '3.7.3 (default, Dec 20 2019, 18:57:59) \\n[GCC 8.3.0]', 'platform': 'Linux-4.19.0-9-amd64-x86_64-with-debian-10.4', 'event': 'prepare_vocab'}\n",
      "2022-02-17 14:08:55,675 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 5517507 word corpus (94.40220850414394%% of original 5844680, drops 327173)', 'datetime': '2022-02-17T14:08:55.675793', 'gensim': '4.1.2', 'python': '3.7.3 (default, Dec 20 2019, 18:57:59) \\n[GCC 8.3.0]', 'platform': 'Linux-4.19.0-9-amd64-x86_64-with-debian-10.4', 'event': 'prepare_vocab'}\n",
      "2022-02-17 14:08:55,844 : INFO : deleting the raw counts dictionary of 280617 items\n",
      "2022-02-17 14:08:55,847 : INFO : sample=0.001 downsamples 43 most-common words\n",
      "2022-02-17 14:08:55,847 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 4268608.194985565 word corpus (77.4%% of prior 5517507)', 'datetime': '2022-02-17T14:08:55.847739', 'gensim': '4.1.2', 'python': '3.7.3 (default, Dec 20 2019, 18:57:59) \\n[GCC 8.3.0]', 'platform': 'Linux-4.19.0-9-amd64-x86_64-with-debian-10.4', 'event': 'prepare_vocab'}\n",
      "2022-02-17 14:08:56,178 : INFO : estimated required memory for 49345 words and 100 dimensions: 64148500 bytes\n",
      "2022-02-17 14:08:56,178 : INFO : resetting layer weights\n",
      "2022-02-17 14:08:56,193 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2022-02-17T14:08:56.193965', 'gensim': '4.1.2', 'python': '3.7.3 (default, Dec 20 2019, 18:57:59) \\n[GCC 8.3.0]', 'platform': 'Linux-4.19.0-9-amd64-x86_64-with-debian-10.4', 'event': 'build_vocab'}\n",
      "2022-02-17 14:08:56,194 : INFO : Word2Vec lifecycle event {'msg': 'training model with 3 workers on 49345 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2022-02-17T14:08:56.194447', 'gensim': '4.1.2', 'python': '3.7.3 (default, Dec 20 2019, 18:57:59) \\n[GCC 8.3.0]', 'platform': 'Linux-4.19.0-9-amd64-x86_64-with-debian-10.4', 'event': 'train'}\n",
      "2022-02-17 14:08:57,199 : INFO : EPOCH 1 - PROGRESS: at 12.87% examples, 555240 words/s, in_qsize 5, out_qsize 0\n",
      "2022-02-17 14:08:58,206 : INFO : EPOCH 1 - PROGRESS: at 25.86% examples, 550858 words/s, in_qsize 5, out_qsize 0\n",
      "2022-02-17 14:08:59,212 : INFO : EPOCH 1 - PROGRESS: at 38.93% examples, 554546 words/s, in_qsize 5, out_qsize 0\n",
      "2022-02-17 14:09:00,213 : INFO : EPOCH 1 - PROGRESS: at 51.85% examples, 555679 words/s, in_qsize 6, out_qsize 0\n",
      "2022-02-17 14:09:01,232 : INFO : EPOCH 1 - PROGRESS: at 65.26% examples, 555462 words/s, in_qsize 5, out_qsize 1\n",
      "2022-02-17 14:09:02,242 : INFO : EPOCH 1 - PROGRESS: at 78.79% examples, 555907 words/s, in_qsize 6, out_qsize 0\n",
      "2022-02-17 14:09:03,248 : INFO : EPOCH 1 - PROGRESS: at 92.00% examples, 557609 words/s, in_qsize 5, out_qsize 0\n",
      "2022-02-17 14:09:03,823 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-02-17 14:09:03,834 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-02-17 14:09:03,835 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-02-17 14:09:03,836 : INFO : EPOCH - 1 : training on 5844680 raw words (4267819 effective words) took 7.6s, 558605 effective words/s\n",
      "2022-02-17 14:09:04,838 : INFO : EPOCH 2 - PROGRESS: at 13.03% examples, 563375 words/s, in_qsize 5, out_qsize 0\n",
      "2022-02-17 14:09:05,844 : INFO : EPOCH 2 - PROGRESS: at 26.46% examples, 566056 words/s, in_qsize 5, out_qsize 0\n",
      "2022-02-17 14:09:06,857 : INFO : EPOCH 2 - PROGRESS: at 39.62% examples, 563451 words/s, in_qsize 5, out_qsize 0\n",
      "2022-02-17 14:09:07,881 : INFO : EPOCH 2 - PROGRESS: at 52.53% examples, 559129 words/s, in_qsize 5, out_qsize 0\n",
      "2022-02-17 14:09:08,884 : INFO : EPOCH 2 - PROGRESS: at 65.79% examples, 558605 words/s, in_qsize 5, out_qsize 0\n",
      "2022-02-17 14:09:09,891 : INFO : EPOCH 2 - PROGRESS: at 79.34% examples, 558779 words/s, in_qsize 5, out_qsize 0\n",
      "2022-02-17 14:09:10,893 : INFO : EPOCH 2 - PROGRESS: at 92.17% examples, 558404 words/s, in_qsize 5, out_qsize 0\n",
      "2022-02-17 14:09:11,453 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-02-17 14:09:11,461 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-02-17 14:09:11,462 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-02-17 14:09:11,462 : INFO : EPOCH - 2 : training on 5844680 raw words (4268349 effective words) took 7.6s, 559781 effective words/s\n",
      "2022-02-17 14:09:12,471 : INFO : EPOCH 3 - PROGRESS: at 12.56% examples, 538768 words/s, in_qsize 5, out_qsize 0\n",
      "2022-02-17 14:09:13,476 : INFO : EPOCH 3 - PROGRESS: at 25.66% examples, 546636 words/s, in_qsize 5, out_qsize 0\n",
      "2022-02-17 14:09:14,495 : INFO : EPOCH 3 - PROGRESS: at 38.54% examples, 546921 words/s, in_qsize 5, out_qsize 0\n",
      "2022-02-17 14:09:15,505 : INFO : EPOCH 3 - PROGRESS: at 51.64% examples, 550590 words/s, in_qsize 5, out_qsize 0\n",
      "2022-02-17 14:09:16,535 : INFO : EPOCH 3 - PROGRESS: at 65.26% examples, 551658 words/s, in_qsize 5, out_qsize 0\n",
      "2022-02-17 14:09:17,543 : INFO : EPOCH 3 - PROGRESS: at 78.79% examples, 552990 words/s, in_qsize 6, out_qsize 0\n",
      "2022-02-17 14:09:18,546 : INFO : EPOCH 3 - PROGRESS: at 91.83% examples, 554210 words/s, in_qsize 5, out_qsize 0\n",
      "2022-02-17 14:09:19,126 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-02-17 14:09:19,129 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-02-17 14:09:19,138 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-02-17 14:09:19,138 : INFO : EPOCH - 3 : training on 5844680 raw words (4268055 effective words) took 7.7s, 556108 effective words/s\n",
      "2022-02-17 14:09:20,147 : INFO : EPOCH 4 - PROGRESS: at 13.03% examples, 560263 words/s, in_qsize 5, out_qsize 0\n",
      "2022-02-17 14:09:21,160 : INFO : EPOCH 4 - PROGRESS: at 26.46% examples, 562316 words/s, in_qsize 5, out_qsize 0\n",
      "2022-02-17 14:09:22,166 : INFO : EPOCH 4 - PROGRESS: at 39.81% examples, 564408 words/s, in_qsize 5, out_qsize 0\n",
      "2022-02-17 14:09:23,179 : INFO : EPOCH 4 - PROGRESS: at 53.11% examples, 564903 words/s, in_qsize 5, out_qsize 0\n",
      "2022-02-17 14:09:24,206 : INFO : EPOCH 4 - PROGRESS: at 66.85% examples, 565143 words/s, in_qsize 5, out_qsize 0\n",
      "2022-02-17 14:09:25,207 : INFO : EPOCH 4 - PROGRESS: at 80.54% examples, 565990 words/s, in_qsize 5, out_qsize 0\n",
      "2022-02-17 14:09:26,207 : INFO : EPOCH 4 - PROGRESS: at 93.77% examples, 566669 words/s, in_qsize 5, out_qsize 0\n",
      "2022-02-17 14:09:26,665 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-02-17 14:09:26,667 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-02-17 14:09:26,676 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-02-17 14:09:26,677 : INFO : EPOCH - 4 : training on 5844680 raw words (4268033 effective words) took 7.5s, 566245 effective words/s\n",
      "2022-02-17 14:09:27,684 : INFO : EPOCH 5 - PROGRESS: at 13.03% examples, 561313 words/s, in_qsize 5, out_qsize 0\n",
      "2022-02-17 14:09:28,685 : INFO : EPOCH 5 - PROGRESS: at 26.46% examples, 566367 words/s, in_qsize 5, out_qsize 0\n",
      "2022-02-17 14:09:29,697 : INFO : EPOCH 5 - PROGRESS: at 39.81% examples, 566135 words/s, in_qsize 5, out_qsize 0\n",
      "2022-02-17 14:09:30,718 : INFO : EPOCH 5 - PROGRESS: at 53.28% examples, 566994 words/s, in_qsize 5, out_qsize 0\n",
      "2022-02-17 14:09:31,725 : INFO : EPOCH 5 - PROGRESS: at 66.65% examples, 565996 words/s, in_qsize 5, out_qsize 0\n",
      "2022-02-17 14:09:32,736 : INFO : EPOCH 5 - PROGRESS: at 80.39% examples, 565811 words/s, in_qsize 5, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-17 14:09:33,738 : INFO : EPOCH 5 - PROGRESS: at 93.38% examples, 565429 words/s, in_qsize 5, out_qsize 0\n",
      "2022-02-17 14:09:34,212 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-02-17 14:09:34,219 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-02-17 14:09:34,220 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-02-17 14:09:34,221 : INFO : EPOCH - 5 : training on 5844680 raw words (4268649 effective words) took 7.5s, 565967 effective words/s\n",
      "2022-02-17 14:09:34,221 : INFO : Word2Vec lifecycle event {'msg': 'training on 29223400 raw words (21340905 effective words) took 38.0s, 561209 effective words/s', 'datetime': '2022-02-17T14:09:34.221506', 'gensim': '4.1.2', 'python': '3.7.3 (default, Dec 20 2019, 18:57:59) \\n[GCC 8.3.0]', 'platform': 'Linux-4.19.0-9-amd64-x86_64-with-debian-10.4', 'event': 'train'}\n",
      "2022-02-17 14:09:34,221 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec(vocab=49345, vector_size=100, alpha=0.025)', 'datetime': '2022-02-17T14:09:34.221922', 'gensim': '4.1.2', 'python': '3.7.3 (default, Dec 20 2019, 18:57:59) \\n[GCC 8.3.0]', 'platform': 'Linux-4.19.0-9-amd64-x86_64-with-debian-10.4', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "text = [t.split() for t,p in train]\n",
    "\n",
    "# the following configuration is the default configuration\n",
    "w2v = gensim.models.word2vec.Word2Vec(sentences=text,\n",
    "                                vector_size=100, window=5,               ### here we train a cbow model \n",
    "                                min_count=5,                      \n",
    "                                sample=0.001, workers=3,\n",
    "                                sg=1, hs=0, negative=5,        ### set sg to 1 to train a sg model\n",
    "                                cbow_mean=1,\n",
    "                                epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load pre-trained embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's for later\n",
    "\n",
    "#from gensim.test.utils import datapath\n",
    "#w2v = KeyedVectors.load_word2vec_format(datapath('downloaded_vectors_path'), binary=False)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Gensim, embeddings are loaded and can be used via the [\"KeyedVectors\"](https://radimrehurek.com/gensim/models/keyedvectors.html) class\n",
    "\n",
    "> Since trained word vectors are independent from the way they were trained (Word2Vec, FastText, WordRank, VarEmbed etc), they can be represented by a standalone structure, as implemented in this module.\n",
    "\n",
    ">The structure is called “KeyedVectors” and is essentially a mapping between entities and vectors. Each entity is identified by its string id, so this is a mapping between {str => 1D numpy array}.\n",
    "\n",
    ">The entity typically corresponds to a word (so the mapping maps words to 1D vectors), but for some models, they key can also correspond to a document, a graph node etc. To generalize over different use-cases, this module calls the keys entities. Each entity is always represented by its string id, no matter whether the entity is a word, a document or a graph node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 2: Test learnt embeddings\n",
    "\n",
    "The word embedding space directly encodes similarities between words: the vector coding for the word \"great\" will be closer to the vector coding for \"good\" than to the one coding for \"bad\". Generally, [cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity) is the distance used when considering distance between vectors.\n",
    "\n",
    "KeyedVectors have a built in [similarity](https://radimrehurek.com/gensim/models /keyedvectors.html#gensim.models.keyedvectors.BaseKeyedVectors.similarity) method to compute the cosine similarity between words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "great and good: 0.7703075\n",
      "great and bad: 0.5083331\n"
     ]
    }
   ],
   "source": [
    "# is great really closer to good than to bad ?\n",
    "print(\"great and good:\",w2v.wv.similarity(\"great\",\"good\"))\n",
    "print(\"great and bad:\",w2v.wv.similarity(\"great\",\"bad\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since cosine distance encodes similarity, neighboring words are supposed to be similar. The [most_similar](https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.BaseKeyedVectors.most_similar) method returns the `topn` words given a query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('film', 0.9380859136581421),\n",
       " ('\"film\"', 0.8366387486457825),\n",
       " ('\"movie\"', 0.8037150502204895),\n",
       " ('movie,', 0.7774391174316406),\n",
       " ('flick', 0.7754345536231995)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The query can be as simple as a word, such as \"movie\"\n",
    "\n",
    "# Try changing the word\n",
    "w2v.wv.most_similar(\"movie\",topn=5) # 5 most similar words\n",
    "#w2v.wv.most_similar(\"awesome\",topn=5)\n",
    "#w2v.wv.most_similar(\"actor\",topn=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But it can be a more complicated query\n",
    "Word embedding spaces tend to encode much more.\n",
    "\n",
    "The most famous exemple is: `vec(king) - vec(man) + vec(woman) => vec(queen)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('awful', 0.7360645532608032),\n",
       " ('crappy', 0.64714515209198),\n",
       " ('abysmal', 0.6406511664390564)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What is awesome - good + bad ?\n",
    "w2v.wv.most_similar(positive=[\"awesome\",\"bad\"],negative=[\"good\"],topn=3)  \n",
    "\n",
    "#w2v.wv.most_similar(positive=[\"actor\",\"woman\"],negative=[\"man\"],topn=3) # do the famous exemple works for actor ?\n",
    "\n",
    "\n",
    "# Try other things like plurals for exemple.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test learnt \"synctactic\" and \"semantic\" similarities, Mikolov et al. introduced a special dataset containing a wide variety of three way similarities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-17 14:24:39,978 : INFO : Evaluating word analogies for top 300000 words in the model on questions-words.txt\n",
      "2022-02-17 14:24:40,196 : INFO : capital-common-countries: 1.3% (2/156)\n",
      "2022-02-17 14:24:40,371 : INFO : capital-world: 1.8% (2/111)\n",
      "2022-02-17 14:24:40,423 : INFO : currency: 0.0% (0/18)\n",
      "2022-02-17 14:24:40,819 : INFO : city-in-state: 0.0% (0/301)\n",
      "2022-02-17 14:24:41,409 : INFO : family: 32.4% (136/420)\n",
      "2022-02-17 14:24:42,557 : INFO : gram1-adjective-to-adverb: 2.1% (18/870)\n",
      "2022-02-17 14:24:43,209 : INFO : gram2-opposite: 2.4% (13/552)\n",
      "2022-02-17 14:24:44,608 : INFO : gram3-comparative: 18.2% (216/1190)\n",
      "2022-02-17 14:24:45,497 : INFO : gram4-superlative: 9.9% (75/756)\n",
      "2022-02-17 14:24:46,459 : INFO : gram5-present-participle: 20.1% (163/812)\n",
      "2022-02-17 14:24:47,545 : INFO : gram6-nationality-adjective: 1.4% (14/967)\n",
      "2022-02-17 14:24:49,053 : INFO : gram7-past-tense: 21.3% (268/1260)\n",
      "2022-02-17 14:24:50,047 : INFO : gram8-plural: 7.1% (58/812)\n",
      "2022-02-17 14:24:50,820 : INFO : gram9-plural-verbs: 29.7% (193/650)\n",
      "2022-02-17 14:24:50,821 : INFO : Quadruplets with out-of-vocabulary words: 54.6%\n",
      "2022-02-17 14:24:50,822 : INFO : NB: analogies containing OOV words were skipped from evaluation! To change this behavior, use \"dummy4unknown=True\"\n",
      "2022-02-17 14:24:50,823 : INFO : Total accuracy: 13.0% (1158/8875)\n"
     ]
    }
   ],
   "source": [
    "out = w2v.wv.evaluate_word_analogies(\"questions-words.txt\",case_insensitive=True)  #original semantic syntactic dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When training the w2v models on the review dataset, since it hasn't been learnt with a lot of data, it does not perform very well. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 3:  sentiment classification\n",
    "\n",
    "In the previous practical session, we used a bag of word approach to transform text into vectors.\n",
    "Here, we propose to try to use word vectors (previously learnt or loaded).\n",
    "\n",
    "\n",
    "### <font color='green'> Since we have only word vectors and that sentences are made of multiple words, we need to aggregate them. </font>\n",
    "\n",
    "\n",
    "### (1) Vectorize reviews using word vectors:\n",
    "\n",
    "Word aggregation can be done in different ways:\n",
    "\n",
    "- Sum\n",
    "- Average\n",
    "- Min/feature\n",
    "- Max/feature\n",
    "\n",
    "#### a few pointers:\n",
    "\n",
    "- `w2v.wv.vocab` is a `set()` of the vocabulary (all existing words in your model)\n",
    "- `np.minimum(a,b) and np.maximum(a,b)` respectively return element-wise min/max "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.60161260e+00  2.34261493e+01  2.85458832e+00 -4.02416524e-01\n",
      " -3.84110824e-01 -1.50540183e+01  1.26395273e+01  2.93201242e+01\n",
      " -3.18219926e+01 -1.93918301e+01  3.04965462e+00 -1.91284421e+01\n",
      "  1.80939776e+00  7.68914065e+00  3.92100169e+00 -1.39483021e+01\n",
      "  7.95380589e+00 -1.02666420e+01 -4.50450445e+00 -5.08434701e+01\n",
      "  1.12570424e+01  9.51207362e+00  1.42607800e+01 -1.59116835e+01\n",
      "  2.92106551e+00 -9.11876570e+00 -1.58846393e+01  1.11142396e+00\n",
      " -1.47880175e+01  1.48105596e+01  3.45700143e+01 -1.11219331e+01\n",
      "  1.41847650e+01 -1.98434904e+01 -8.46705215e+00  1.12777096e+01\n",
      " -8.24183159e-01  1.12461092e+01 -9.12432173e+00 -1.47258999e+01\n",
      "  1.99210619e+01 -1.45931017e+01 -1.08363237e+01 -1.57887857e-01\n",
      "  1.76077756e+01  6.97343782e+00 -1.54449440e+01 -5.74768056e+00\n",
      "  1.96426683e+01  9.89441384e+00  7.35843183e+00 -2.61774646e+01\n",
      "  3.84610711e+00 -2.16774679e+00 -1.08373068e+01  1.45704646e+01\n",
      "  1.45386306e+01  4.16796077e+00 -1.97490049e+01  2.92674820e+00\n",
      "  9.02114664e+00 -4.24425074e+00  2.05641389e+01  8.15031725e+00\n",
      " -1.86698912e+01  1.94456007e+01 -8.38709529e-04  1.10619208e+01\n",
      " -2.71645134e+01  1.04311805e+01 -3.34686211e+00  1.59806856e+01\n",
      "  1.58173034e+01 -6.10959523e+00  2.05633343e+01 -1.27100462e+01\n",
      "  1.20649992e+01 -1.07500571e+01 -1.00937884e+01  1.64993647e+00\n",
      " -1.21254149e+01 -6.46083718e+00 -2.10152099e+01  2.23467006e+01\n",
      " -6.38250481e+00 -1.52494692e+01  6.93904183e+00  1.21643466e+01\n",
      "  2.05542897e+01  1.00188120e+01  3.96777000e+01  3.79885455e+00\n",
      "  2.15955792e-01 -2.26709802e+00  2.99247621e+01 -9.46546313e-02\n",
      "  5.17320216e+00 -2.83498130e+01  9.71425877e+00 -9.26811575e+00]\n",
      "[-1.60161260e+00  2.34261493e+01  2.85458832e+00 -4.02416524e-01\n",
      " -3.84110824e-01 -1.50540183e+01  1.26395273e+01  2.93201242e+01\n",
      " -3.18219926e+01 -1.93918301e+01  3.04965462e+00 -1.91284421e+01\n",
      "  1.80939776e+00  7.68914065e+00  3.92100169e+00 -1.39483021e+01\n",
      "  7.95380589e+00 -1.02666420e+01 -4.50450445e+00 -5.08434701e+01\n",
      "  1.12570424e+01  9.51207362e+00  1.42607800e+01 -1.59116835e+01\n",
      "  2.92106551e+00 -9.11876570e+00 -1.58846393e+01  1.11142396e+00\n",
      " -1.47880175e+01  1.48105596e+01  3.45700143e+01 -1.11219331e+01\n",
      "  1.41847650e+01 -1.98434904e+01 -8.46705215e+00  1.12777096e+01\n",
      " -8.24183159e-01  1.12461092e+01 -9.12432173e+00 -1.47258999e+01\n",
      "  1.99210619e+01 -1.45931017e+01 -1.08363237e+01 -1.57887857e-01\n",
      "  1.76077756e+01  6.97343782e+00 -1.54449440e+01 -5.74768056e+00\n",
      "  1.96426683e+01  9.89441384e+00  7.35843183e+00 -2.61774646e+01\n",
      "  3.84610711e+00 -2.16774679e+00 -1.08373068e+01  1.45704646e+01\n",
      "  1.45386306e+01  4.16796077e+00 -1.97490049e+01  2.92674820e+00\n",
      "  9.02114664e+00 -4.24425074e+00  2.05641389e+01  8.15031725e+00\n",
      " -1.86698912e+01  1.94456007e+01 -8.38709529e-04  1.10619208e+01\n",
      " -2.71645134e+01  1.04311805e+01 -3.34686211e+00  1.59806856e+01\n",
      "  1.58173034e+01 -6.10959523e+00  2.05633343e+01 -1.27100462e+01\n",
      "  1.20649992e+01 -1.07500571e+01 -1.00937884e+01  1.64993647e+00\n",
      " -1.21254149e+01 -6.46083718e+00 -2.10152099e+01  2.23467006e+01\n",
      " -6.38250481e+00 -1.52494692e+01  6.93904183e+00  1.21643466e+01\n",
      "  2.05542897e+01  1.00188120e+01  3.96777000e+01  3.79885455e+00\n",
      "  2.15955792e-01 -2.26709802e+00  2.99247621e+01 -9.46546313e-02\n",
      "  5.17320216e+00 -2.83498130e+01  9.71425877e+00 -9.26811575e+00]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# We first need to vectorize text:\n",
    "# First we propose to a sum of them\n",
    "\n",
    "\n",
    "def vectorize(text,mean=False):\n",
    "    \"\"\"\n",
    "This function should vectorize one review\n",
    "\n",
    "input: str\n",
    "output: np.array(float)\n",
    "\"\"\" \n",
    "    out = np.zeros(w2v.wv.vector_size)\n",
    "    words = text.split()\n",
    "\n",
    "    for word in words:\n",
    "        if word in w2v.wv.key_to_index:\n",
    "            out += w2v.wv.get_vector(word)\n",
    "    \n",
    "    if mean:\n",
    "        out = out /len(out) \n",
    "    \n",
    "    return np.array(out)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "classes = [pol for text,pol in train]\n",
    "X = [vectorize(text) for text,pol in train]\n",
    "print(X[0])\n",
    "X_test = [vectorize(text) for text,pol in test]\n",
    "true = [pol for text,pol in test]\n",
    "\n",
    "#let's see what a review vector looks like.\n",
    "print(X[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Train a classifier \n",
    "as in the previous practical session, train a logistic regression to do sentiment classification with word vectors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([-4.34236219e+00,  3.62662455e+01,  4.14375157e+00, -4.22486309e+00,\n",
      "        7.82260588e-01, -3.00393723e+01,  2.00649612e+01,  5.78603410e+01,\n",
      "       -5.87014027e+01, -3.62172614e+01,  3.88868139e+00, -2.26265997e+01,\n",
      "       -5.95832126e+00,  1.25780692e+01,  8.89782955e-01, -1.50965601e+01,\n",
      "        5.08506664e+00, -2.05852479e+01, -1.18229582e+01, -8.32876473e+01,\n",
      "        9.37544894e+00,  1.40406474e+01,  2.79653832e+01, -3.40109885e+01,\n",
      "        8.11723651e-02, -7.39161443e+00, -3.50620998e+01, -8.86587314e-01,\n",
      "       -2.12490439e+01,  2.64194772e+01,  5.10030558e+01, -1.92979023e+01,\n",
      "        2.82766312e+01, -2.04660170e+01, -1.90326637e+01,  2.45674130e+01,\n",
      "       -5.28498378e+00,  2.06663259e+01, -2.33218443e+01, -2.55065903e+01,\n",
      "        3.45070483e+01, -1.30247758e+01, -2.07453915e+01,  7.27442256e-02,\n",
      "        3.69683459e+01,  1.10841353e+01, -1.59619751e+01, -1.51544273e+01,\n",
      "        4.66436658e+01,  1.98787002e+01,  9.25008722e+00, -4.43417702e+01,\n",
      "        7.49701840e+00, -2.73225786e+00, -5.54244543e+00,  2.49417487e+01,\n",
      "        2.09308784e+01,  1.14540866e+01, -2.49911549e+01,  1.23737676e+01,\n",
      "        1.31887175e+01, -1.36522836e+01,  4.09193207e+01,  4.85778282e+00,\n",
      "       -3.57488253e+01,  3.70259558e+01, -4.61031092e+00,  1.72943123e+01,\n",
      "       -4.62025961e+01,  1.84428401e+01, -7.70018896e+00,  2.56585827e+01,\n",
      "        2.10070891e+01,  5.32451115e+00,  3.13915696e+01, -2.07127716e+01,\n",
      "        1.76941367e+01, -1.93155470e+01, -1.94421774e+01, -1.39800002e+01,\n",
      "       -1.66833308e+01,  1.41194135e-01, -2.86111305e+01,  3.83600806e+01,\n",
      "       -3.05810353e+00, -2.28840971e+01,  7.26005462e+00,  2.41682766e+01,\n",
      "        4.21413071e+01,  1.60747094e+01,  7.39753149e+01, -3.05005126e+00,\n",
      "        1.09227658e+01, -1.14059233e+01,  5.31743221e+01,  4.37181841e+00,\n",
      "        1.15278418e+01, -4.35531856e+01,  2.16405734e+01, -1.82582266e+01]), array([-16.00999223,  27.55072648,   2.18913389,  -6.06722506,\n",
      "        -2.41273266, -39.00922635,  32.45116217,  62.11967634,\n",
      "       -55.04090493, -27.65702249,  -6.36568172, -40.1442004 ,\n",
      "        -1.04492923,  21.14090044,  -2.04594817, -30.02347843,\n",
      "        20.24509674, -32.70479521,  -4.67164493, -90.53435424,\n",
      "        22.90616153,  21.3351458 ,  39.63467606, -36.77354696,\n",
      "        -4.82305263,  -8.60776161, -39.37991973,   4.09236071,\n",
      "       -32.95929165,  40.06875041,  71.60289146, -26.1732187 ,\n",
      "        34.39798995, -38.77272165, -25.13222854,  22.5440846 ,\n",
      "       -12.9811619 ,  27.37660988, -38.97018249, -37.27282114,\n",
      "        47.71985899, -10.78159169, -23.93321163,  -1.19588106,\n",
      "        42.48009055,   9.1622081 , -25.29176452,  -7.54934411,\n",
      "        43.7336827 ,  21.60104794,   3.64476536, -48.11619068,\n",
      "        11.39470188,   1.01201985, -15.30455639,  23.0176866 ,\n",
      "        24.18081685,   7.71000291, -24.7605564 ,  12.25662622,\n",
      "        16.70012634, -11.10060644,  39.55881506,  10.45388276,\n",
      "       -47.65750546,  48.10120429,   7.30586646,   7.18378568,\n",
      "       -58.924034  ,  31.31660255, -22.26650412,  23.86269438,\n",
      "        24.87426474,  -4.53221305,  42.91616317, -34.01807289,\n",
      "        19.30671894,  -9.92300977, -32.72978264, -20.18517682,\n",
      "       -20.07982516,  -7.93181717, -27.46383323,  44.68385335,\n",
      "        -7.73060341, -35.21790073,   9.70326823,  33.46600502,\n",
      "        40.485749  ,  11.75654787,  87.157668  ,   3.50572681,\n",
      "        13.99877428, -16.27226477,  61.71401917,  15.11474944,\n",
      "        20.42116155, -40.19640273,  19.75732324, -18.29195034]), array([-1.54658562e+01,  2.41307728e+01,  2.41799608e+00, -4.02926362e-01,\n",
      "        8.90155144e-01, -2.27268185e+01,  2.30454187e+01,  5.46017339e+01,\n",
      "       -5.14957207e+01, -2.13460208e+01,  5.37609880e+00, -2.73073601e+01,\n",
      "        2.67123981e+00,  1.95916271e+01,  7.37512229e-01, -2.21799571e+01,\n",
      "        7.36899495e+00, -2.46374539e+01, -8.53307473e+00, -7.60065132e+01,\n",
      "        1.97389731e+01,  1.44673165e+01,  2.45797533e+01, -2.73800693e+01,\n",
      "       -1.90824844e+00, -1.28310918e+01, -2.93096710e+01,  1.25286618e+01,\n",
      "       -2.17396355e+01,  3.37252187e+01,  5.18213478e+01, -1.85619752e+01,\n",
      "        2.49370181e+01, -2.90542380e+01, -1.92815767e+01,  1.96725041e+01,\n",
      "       -3.64218436e+00,  2.69130948e+01, -2.63561810e+01, -3.02033511e+01,\n",
      "        3.49051350e+01, -7.25342732e+00, -2.32622132e+01,  6.78610147e+00,\n",
      "        2.38639344e+01,  6.20378907e+00, -2.82188315e+01, -1.03066582e+01,\n",
      "        3.80002685e+01,  1.15921621e+01,  2.25236719e+00, -3.67457692e+01,\n",
      "        7.33552424e-02,  2.19755092e+00, -3.28472748e+00,  1.15107959e+01,\n",
      "        1.66965709e+01,  1.46984755e+00, -2.88136261e+01,  3.88520477e-01,\n",
      "        8.02777784e+00, -1.56603716e+01,  4.10344147e+01,  8.70164110e+00,\n",
      "       -3.34778228e+01,  3.41833463e+01,  7.01654193e+00,  1.07938546e+01,\n",
      "       -5.12410645e+01,  2.42584583e+01, -7.70531568e+00,  2.14225087e+01,\n",
      "        2.04869707e+01, -7.50392652e-01,  2.96068562e+01, -2.05887676e+01,\n",
      "        2.86020388e+01, -7.31574428e+00, -2.52235799e+01, -1.20001356e+01,\n",
      "       -1.83044079e+01, -1.15234682e+01, -3.62037650e+01,  3.25712419e+01,\n",
      "       -1.15805265e+01, -2.68233717e+01,  9.50871254e+00,  1.88663761e+01,\n",
      "        3.00487667e+01,  1.20611896e+01,  6.93504813e+01,  2.35990359e+00,\n",
      "       -2.23959083e+00, -1.33083837e+01,  4.96099157e+01,  5.14697020e+00,\n",
      "        4.52622207e+00, -3.33666774e+01,  1.70292555e+01, -1.96708996e+01]), array([ -13.68633185,   72.04130658,   12.314143  ,    6.22254738,\n",
      "         20.3372452 ,  -76.18932607,   44.73387585,  149.09408978,\n",
      "       -140.84246427,  -70.85984263,   15.50534032,  -71.4313103 ,\n",
      "        -14.87418933,   32.09849828,    7.07844465,  -59.96709512,\n",
      "         17.34956415,  -69.00426308,  -14.44042177, -216.14988076,\n",
      "         30.65418587,   28.28989874,   76.20503346,  -84.200109  ,\n",
      "         -4.71773202,  -23.51461864,  -86.97660619,   30.69095069,\n",
      "        -70.26538242,   72.09210235,  150.94333947,  -65.76457639,\n",
      "         74.4914268 ,  -88.9891284 ,  -51.45450908,   53.9594404 ,\n",
      "         -2.8448791 ,   55.29154681,  -75.7299806 ,  -73.64368436,\n",
      "         84.44099011,  -15.97518057,  -65.95424977,   14.59709293,\n",
      "        101.10906126,   30.85756146,  -48.66955774,  -32.65286552,\n",
      "         99.81609039,   47.98893255,   30.58721922, -106.82040976,\n",
      "         16.361459  ,   -1.3500966 ,  -26.3365584 ,   35.1113608 ,\n",
      "         58.76096887,   14.85668532,  -68.67838495,    8.7142862 ,\n",
      "         42.50517244,  -36.12946325,  106.49653789,   23.38574312,\n",
      "        -90.75196402,  111.88848618,   16.97652215,   52.8684865 ,\n",
      "       -148.76388373,   65.24141674,  -12.70593263,   71.96412986,\n",
      "         58.11017383,   -5.31193873,   82.5704195 ,  -42.11280858,\n",
      "         70.79719903,  -26.29350507,  -73.26817493,  -39.06333762,\n",
      "        -50.91309832,  -16.15792649,  -92.40064623,  102.08530352,\n",
      "        -14.83867397,  -70.29513016,   35.56200893,   38.24845548,\n",
      "         95.2878075 ,   35.71260647,  175.75630636,  -17.0314045 ,\n",
      "          9.91582872,  -27.92412669,  149.04767187,   23.56631816,\n",
      "         32.95706419,  -87.44633031,   47.23721289,  -38.62658974]), array([-7.25079054e+00,  2.50465480e+01,  2.60436980e+00,  1.97718532e+00,\n",
      "       -4.38404917e-02, -2.37753211e+01,  1.17499827e+01,  3.65813936e+01,\n",
      "       -3.73609353e+01, -1.82043881e+01,  2.65034223e+00, -2.41189960e+01,\n",
      "       -3.38638186e-01,  8.14766642e+00,  2.97989530e+00, -1.90527479e+01,\n",
      "        1.06791833e+01, -1.93676835e+01, -4.54826328e+00, -5.37011828e+01,\n",
      "        1.30684998e+01,  6.63692960e+00,  2.18062411e+01, -2.18381413e+01,\n",
      "       -1.07248848e+00, -7.73262939e+00, -1.77280232e+01,  7.78897588e+00,\n",
      "       -2.11362268e+01,  1.57926144e+01,  3.54420361e+01, -1.66331305e+01,\n",
      "        1.43296232e+01, -2.01312058e+01, -1.34685898e+01,  1.59646813e+01,\n",
      "       -3.10974998e+00,  1.00771946e+01, -1.94158166e+01, -1.69430919e+01,\n",
      "        2.19870328e+01, -1.01250737e+01, -1.67652090e+01, -1.70181999e-02,\n",
      "        2.56129426e+01,  4.48658959e+00, -1.48692978e+01,  1.18527261e+00,\n",
      "        2.56101964e+01,  1.97476221e+01,  3.72673769e+00, -2.55628227e+01,\n",
      "        3.61003644e+00,  1.08453540e+00, -1.62440573e+01,  1.81711026e+01,\n",
      "        1.96985829e+01,  5.22282989e+00, -1.90692255e+01,  6.44191244e+00,\n",
      "        1.58682794e+01, -6.14933261e+00,  2.80975093e+01,  6.61436353e+00,\n",
      "       -3.05604528e+01,  3.38755988e+01,  7.45511685e-01,  1.45915151e+01,\n",
      "       -3.75211104e+01,  1.68941204e+01, -2.83690971e+00,  1.59227590e+01,\n",
      "        1.65393970e+01, -1.51719910e+00,  2.44280414e+01, -1.34325475e+01,\n",
      "        1.28558064e+01, -7.95238616e-01, -2.29069493e+01, -7.16520498e+00,\n",
      "       -1.41424432e+01, -3.64680838e+00, -2.66272796e+01,  2.33778711e+01,\n",
      "       -1.29378206e+00, -1.42410591e+01,  1.13265834e+01,  1.38410367e+01,\n",
      "        2.74843508e+01,  1.32537135e+01,  4.31419149e+01,  1.82975159e-01,\n",
      "        2.02217089e+00, -1.22993461e+01,  3.83590536e+01,  4.11700379e+00,\n",
      "        1.07701248e+01, -3.00269565e+01,  8.93211054e+00, -1.53559722e+01]), array([ -8.12532592,  32.98287328,   3.19156282,  -7.29109737,\n",
      "        -1.16167617, -23.71940109,  22.61638495,  44.7702635 ,\n",
      "       -40.14287741, -20.30580171,  -1.1806072 , -34.09638502,\n",
      "        -2.19154219,   7.60586241,   5.48904522, -18.00344651,\n",
      "        11.93918711, -28.49731406,  -3.24357882, -67.14442285,\n",
      "        16.95393762,  17.35318303,  20.9696353 , -23.53110911,\n",
      "         1.68610129, -18.55290779, -19.93936863,   1.1283348 ,\n",
      "       -31.03274643,  14.70197168,  48.20796788, -16.42898459,\n",
      "        18.68834586, -38.50037445, -16.81705357,  23.42574464,\n",
      "         2.15632296,   8.37041387, -16.6536695 , -17.17360593,\n",
      "        27.9003837 , -11.64383124, -16.67856459,  -1.77759379,\n",
      "        29.95516872,   7.58638196, -11.52507874,  -0.65680307,\n",
      "        22.52634733,  16.95585918,  12.21691961, -32.17253193,\n",
      "        -7.98842347,  -2.99193477, -20.58595871,  12.40858624,\n",
      "        22.2691406 ,  10.11811071, -23.21417692,  11.93055033,\n",
      "         7.74994942,   3.72963265,  30.15020386,   6.68901431,\n",
      "       -26.68081261,  33.30152027,   6.2523095 ,   6.86440748,\n",
      "       -39.91589375,  13.93572119, -10.25476925,  20.84642489,\n",
      "        21.6847719 ,  -8.4658766 ,  23.41641407, -16.32494095,\n",
      "        15.24680568,  -0.44110361, -19.13913579,  -3.02009404,\n",
      "       -20.09433327,  -3.84517746, -33.48451669,  33.13872271,\n",
      "        -3.50704159, -18.09987948,   8.39807553,  19.4822541 ,\n",
      "        27.42457459,  14.01311171,  50.66285247,   2.53976334,\n",
      "        -0.66805096, -10.04822854,  57.04603356,   7.67763098,\n",
      "        17.77449581, -32.29105508,  12.07475375, -12.58792606]), array([  -8.82407749,   54.653822  ,    8.92818371,    4.98477756,\n",
      "          7.5051191 ,  -59.25055184,   41.67338616,   91.74062561,\n",
      "       -106.2607352 ,  -50.2799182 ,   14.6628597 ,  -54.38964493,\n",
      "        -10.4214666 ,   24.3530335 ,    7.99446754,  -35.48576905,\n",
      "          7.75976575,  -44.82973848,   -8.97979316, -148.39377895,\n",
      "         29.18605281,   22.65554621,   58.95432423,  -48.11435195,\n",
      "         -3.56182763,   -6.12034599,  -52.18656824,    4.35223831,\n",
      "        -57.12351974,   56.83030792,  104.32865497,  -30.81950845,\n",
      "         53.55602565,  -42.37678862,  -33.07729842,   42.82054969,\n",
      "          0.33437796,   36.86224124,  -49.0873711 ,  -60.10317794,\n",
      "         53.52946641,  -24.21755086,  -47.29641639,    5.34962906,\n",
      "         59.41850221,   18.73401134,  -41.13555964,  -38.58733024,\n",
      "         83.23296238,   41.15912179,   21.13732724,  -73.10810957,\n",
      "         12.04774411,   -8.82146636,  -15.70738113,   35.60455548,\n",
      "         34.60739639,    2.60529522,  -49.33225682,   14.68468427,\n",
      "         40.38743943,  -23.21645243,   85.0681825 ,   22.1469043 ,\n",
      "        -70.78605362,   90.2124934 ,    8.04335676,   39.23228331,\n",
      "        -99.6309976 ,   42.88051132,   -3.75291353,   50.72585279,\n",
      "         50.66658828,    4.70207392,   62.52571333,  -33.60163381,\n",
      "         57.38284954,  -29.36147436,  -44.18264834,  -25.39089115,\n",
      "        -22.1265374 ,  -13.40624936,  -61.6101521 ,   71.79518474,\n",
      "        -21.09003053,  -49.96189224,   26.67941499,   41.48342869,\n",
      "         69.09172676,   33.79758752,  126.21296647,   -2.13024303,\n",
      "          2.64732945,  -30.75482081,  100.22262213,   22.49231791,\n",
      "         15.06387487,  -69.01234013,   31.89673846,  -21.21191993]), array([-9.43645879e+00,  3.35472228e+01,  5.56114356e+00, -3.96768825e+00,\n",
      "        6.27233349e-01, -3.10539625e+01,  2.38457256e+01,  6.47308148e+01,\n",
      "       -3.91747248e+01, -2.82769234e+01, -2.68185838e+00, -4.55609971e+01,\n",
      "       -9.69650503e+00,  6.76554833e+00,  5.77134289e+00, -3.01612250e+01,\n",
      "        1.28990632e+01, -2.86261013e+01,  3.13225750e+00, -7.70230603e+01,\n",
      "        1.72272639e+01,  1.70755823e+01,  2.62916974e+01, -3.37973294e+01,\n",
      "        1.13190759e+00, -1.16908352e+01, -3.59178665e+01,  1.15254491e+01,\n",
      "       -2.88410383e+01,  1.65823844e+01,  6.01823320e+01, -1.85195222e+01,\n",
      "        2.16499013e+01, -3.66525850e+01, -2.02080592e+01,  1.62568245e+01,\n",
      "        5.47014156e+00,  9.95749895e+00, -2.89274264e+01, -2.88766235e+01,\n",
      "        3.14759470e+01, -1.59985281e+01, -1.91136781e+01, -9.95551368e+00,\n",
      "        4.06895548e+01,  4.62490617e+00, -2.30608447e+01,  5.34761654e+00,\n",
      "        3.57064202e+01,  2.63287511e+01,  1.70085636e+01, -3.96857235e+01,\n",
      "       -3.08580031e+00, -1.29646481e+01, -2.84356905e+01,  1.88334240e+01,\n",
      "        1.87875804e+01,  1.19597984e+01, -2.21705719e+01,  1.06873794e+01,\n",
      "        1.15853350e+01, -1.51078118e+01,  3.88944923e+01,  1.87439064e+01,\n",
      "       -3.75709454e+01,  4.03613425e+01, -2.17739811e+00,  2.09729183e+01,\n",
      "       -5.88258546e+01,  2.79916287e+01, -2.86032408e+00,  2.65612477e+01,\n",
      "        3.02315693e+01, -7.11970022e+00,  3.66709380e+01, -2.53314535e+01,\n",
      "        1.89033868e+01,  1.14054376e-02, -3.43210216e+01, -1.11640971e+01,\n",
      "       -3.44744845e+01, -7.34637142e+00, -3.86197961e+01,  4.32149688e+01,\n",
      "        2.15262185e+00, -1.93382924e+01,  1.64452049e+01,  2.04473722e+01,\n",
      "        3.79553446e+01,  1.23721184e+01,  7.02336342e+01, -1.30687016e+00,\n",
      "       -9.79821164e-01, -1.18423990e+01,  6.42217131e+01,  1.55966409e+01,\n",
      "        2.00514064e+01, -3.69983137e+01,  2.74296116e+01, -1.11995548e+01]), array([ -13.14686606,   34.16511017,    7.86148284,   -2.71820285,\n",
      "         17.05718439,  -56.13879034,   30.95662841,  110.07137605,\n",
      "        -73.19963208,  -39.20457581,   21.06525204,  -73.87320579,\n",
      "        -16.66175474,   45.66968348,  -11.56635666,  -41.35137914,\n",
      "         38.52804745,  -53.21020916,   -6.77539472, -143.32487973,\n",
      "         20.63791945,   39.10936741,   65.32546142,  -55.68816436,\n",
      "          8.25828783,   -5.39613639,  -36.24719088,   15.99608769,\n",
      "        -65.87544429,   56.73926025,  113.78356437,  -42.07580371,\n",
      "         50.53242151,  -69.82383089,  -30.06552936,   27.93434837,\n",
      "          1.45584337,   25.87906894,  -42.07113194,  -65.34354167,\n",
      "         53.11593232,  -28.37231893,  -49.2926837 ,   -3.24929145,\n",
      "         69.54179638,   15.61620094,  -44.48355673,  -15.60606841,\n",
      "         72.85516724,   42.03568525,   30.29750763,  -68.72597225,\n",
      "          2.48358521,   -0.89586011,  -24.93450045,   31.45476232,\n",
      "         39.04991535,    3.91229661,  -52.68974161,   17.77055164,\n",
      "         15.40221385,  -28.65210834,   61.67276441,   16.08290381,\n",
      "        -75.09384138,   67.31840621,    1.44299831,   38.70674485,\n",
      "        -96.53326828,   35.03325131,  -16.07890049,   54.50630719,\n",
      "         42.4306916 ,  -17.35953059,   75.28548884,  -55.85505829,\n",
      "         30.74011964,  -20.64335576,  -45.69638901,  -23.33205103,\n",
      "        -48.67113484,  -28.31712814,  -60.82635827,   63.9529717 ,\n",
      "         -1.70304377,  -48.96140957,   36.47317491,   39.23583765,\n",
      "         74.04918631,   33.64467916,  122.83724434,    7.00243438,\n",
      "         22.13733456,  -24.45721971,  110.49018693,   25.44250142,\n",
      "         26.89808645,  -74.13793706,   36.16062006,   -5.62657791]), array([ -5.18276653,  17.8798331 ,  -2.53076498,  -1.47111997,\n",
      "         2.77545576, -11.90460737,   8.85902639,  26.84025791,\n",
      "       -21.56517689, -15.07621106,  -0.89823544, -18.42809468,\n",
      "         1.51103023,   4.37135859,   1.83528342, -13.39099324,\n",
      "         2.50947389, -16.0134894 ,  -2.38331117, -30.22526985,\n",
      "         6.12418737,   7.05639367,  16.15340519, -11.92301902,\n",
      "         1.1260949 , -11.11718753, -13.0810141 ,   6.3093248 ,\n",
      "       -16.67107985,   8.51852459,  25.34792654,  -9.41024749,\n",
      "         7.30781724, -18.8022017 ,  -3.49853468,  10.44812027,\n",
      "        -1.21521188,   9.02082074,  -8.5565147 ,  -9.86121359,\n",
      "         9.7298791 ,  -7.08451201,  -8.78079027,   1.70429053,\n",
      "        21.03331979,   8.41675855,  -3.81281952,   3.86960717,\n",
      "        12.59773731,  13.59033602,   9.29019013, -16.91500215,\n",
      "        -2.4095517 ,  -1.60156548, -10.56734555,   8.6658227 ,\n",
      "         8.71965239,   6.32801434, -14.01922497,   4.06476121,\n",
      "         5.83736062,  -2.64774948,  14.28900335,   1.28064022,\n",
      "       -11.10233998,  18.66361516,   2.038585  ,  12.58054934,\n",
      "       -21.38027032,  10.61094276,  -3.63509722,  12.55412334,\n",
      "        12.41594951,  -0.75975208,  13.20301104,  -5.57047338,\n",
      "         6.77620487,  -1.65431184,  -9.35089216,  -1.92974687,\n",
      "       -11.04247602,  -3.027904  , -18.77175745,  13.18334432,\n",
      "        -1.1100679 ,  -8.82030921,   4.16674628,   5.69108499,\n",
      "        16.51708679,   4.78749955,  30.51279552,   0.29336498,\n",
      "        -0.36483646,  -4.98132831,  24.40127515,   0.66801136,\n",
      "         7.27092545, -14.81960165,   4.34234012,  -4.28126818])]\n"
     ]
    }
   ],
   "source": [
    "print(X[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Regression accuracy with sum:  0.82112\n",
      " Regression accuracy with mean:  0.82044\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "\n",
    "# Scikit Logistic Regression\n",
    "model_log = LogisticRegression(random_state=0,solver='liblinear')\n",
    "model_log.fit(X, classes)\n",
    "\n",
    "pred_model = model_log.predict(X_test)\n",
    "print(\" Regression accuracy with sum: \",accuracy_score(true, pred_model))\n",
    "X_mean = [vectorize(text,True) for text,pol in train]\n",
    "\n",
    "X_test_mean = [vectorize(text,True) for text,pol in test]\n",
    "\n",
    "\n",
    "model_log_mean =  LogisticRegression(random_state=0,solver='liblinear')\n",
    "model_log_mean.fit(X_mean, classes)\n",
    "pred_model_mean = model_log.predict(X_test_mean)\n",
    "print(\" Regression accuracy with mean: \",accuracy_score(true, pred_model_mean))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "performance should be worst than with bag of word (~80%). Sum/Mean aggregation does not work well on long reviews (especially with many frequent words). This adds a lot of noise.\n",
    "\n",
    "## **Todo** :  Try answering the following questions:\n",
    "\n",
    "- Which word2vec model works best: skip-gram or cbow\n",
    "- Do pretrained vectors work best than those learnt on the train dataset ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**(Bonus)** To have a better accuracy, we could try two things:\n",
    "- Better aggregation methods (weight by tf-idf ?)\n",
    "- Another word vectorizing method such as [fasttext](https://radimrehurek.com/gensim/models/fasttext.html)\n",
    "- A document vectorizing method such as [Doc2Vec](https://radimrehurek.com/gensim/models/doc2vec.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --- Generate text with a recurrent neural network (Pytorch) ---\n",
    "### (Mostly Read & Run)\n",
    "\n",
    "The goal is to replicate the (famous) experiment from [Karpathy's blog](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n",
    "\n",
    "To learn to generate text, we train a recurrent neural network to do the following task:\n",
    "\n",
    "Given a \"chunk\" of text: `this is random text`\n",
    "\n",
    "the goal of the network is to predict each character in **`his is random text` ** sequentially given the following sequential input **`this is random tex`**:\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Input ->  Output\n",
    "--------------\n",
    "T    ->    H\n",
    "H    ->    I\n",
    "I    ->    S\n",
    "S    ->    \" \"\n",
    "\" \"  ->    I\n",
    "I    ->    S\n",
    "S    ->    \" \"\n",
    "[...]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Load text (dataset/input.txt)\n",
    "\n",
    "Before building training batch, we load the full text in RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting Unidecode\n",
      "  Downloading Unidecode-1.3.2-py3-none-any.whl (235 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.7/235.7 KB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[33mWARNING: Error parsing requirements for wordcloud: [Errno 2] Aucun fichier ou dossier de ce type: '/users/nfs/Etu3/21113733/.local/lib/python3.7/site-packages/wordcloud-1.8.1.dist-info/METADATA'\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: Unidecode\n",
      "\u001b[33m  WARNING: The script unidecode is installed in '/users/Etu3/21113733/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed Unidecode-1.3.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install Unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file_len = 1115394\n"
     ]
    }
   ],
   "source": [
    "import unidecode\n",
    "import string\n",
    "import random\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "all_characters = string.printable\n",
    "n_characters = len(all_characters)\n",
    "\n",
    "file = unidecode.unidecode(open('input.txt').read()) #clean text => only ascii\n",
    "file_len = len(file)\n",
    "print('file_len =', file_len)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2: Helper functions:\n",
    "\n",
    "We have a text and we want to feed batch of chunks to a neural network:\n",
    "\n",
    "one chunk  A,B,C,D,E\n",
    "[input] A,B,C,D -> B,C,D,E [output]\n",
    "\n",
    "Note: we will use an embedding layer instead of a one-hot encoding scheme.\n",
    "\n",
    "for this, we have 3 functions:\n",
    "\n",
    "- One to get a random str chunk of size `chunk_len` : `random_chunk` \n",
    "- One to turn a chunk into a tensor of size `(1,chunk_len)` coding for each characters : `char_tensor`\n",
    "- One to return random input and output chunks of size `(batch_size,chunk_len)` : `random_training_set`\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[32, 28, 94, 17, 14, 94, 18, 28, 82, 96],\n",
      "        [23, 29, 77, 94, 18, 15, 94, 34, 24, 30],\n",
      "        [77, 96, 50, 73, 94, 16, 18, 31, 14, 94],\n",
      "        [14, 27, 15, 30, 21, 75, 96, 96, 42, 47]]), tensor([[28, 94, 17, 14, 94, 18, 28, 82, 96, 96],\n",
      "        [29, 77, 94, 18, 15, 94, 34, 24, 30, 94],\n",
      "        [96, 50, 73, 94, 16, 18, 31, 14, 94, 22],\n",
      "        [27, 15, 30, 21, 75, 96, 96, 42, 47, 50]]))\n"
     ]
    }
   ],
   "source": [
    "import time, math\n",
    "\n",
    "\n",
    "#Get a piece of text\n",
    "def random_chunk(chunk_len):\n",
    "    start_index = random.randint(0, file_len - chunk_len)\n",
    "    end_index = start_index + chunk_len + 1\n",
    "    return file[start_index:end_index]\n",
    "\n",
    "\n",
    "# Turn string into list of longs\n",
    "def char_tensor(string):\n",
    "    tensor = torch.zeros(1,len(string)).long()\n",
    "    for c in range(len(string)):\n",
    "        tensor[0,c] = all_characters.index(string[c])\n",
    "    return tensor\n",
    "\n",
    "\n",
    "#Turn a piece of text in train/test\n",
    "def random_training_set(chunk_len=200, batch_size=8):\n",
    "    chunks = [random_chunk(chunk_len) for _ in range(batch_size)]\n",
    "    inp = torch.cat([char_tensor(chunk[:-1]) for chunk in chunks],dim=0)\n",
    "    target = torch.cat([char_tensor(chunk[1:]) for chunk in chunks],dim=0)\n",
    "    \n",
    "    return inp, target\n",
    "\n",
    "print(random_training_set(10,4))  ## should return 8 chunks of 10 letters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The actual RNN model (only thing to complete):\n",
    "\n",
    "It should be composed of three distinct modules:\n",
    "\n",
    "- an [embedding layer](https://pytorch.org/docs/stable/nn.html#embedding) (n_characters, hidden_size)\n",
    "\n",
    "```\n",
    "nn.Embedding(len_dic,size_vec)\n",
    "```\n",
    "- a [recurrent](https://pytorch.org/docs/stable/nn.html#recurrent-layers) layer (hidden_size, hidden_size)\n",
    "```\n",
    "nn.RNN(in_size,out_size) or nn.GRU() or nn.LSTM() => rnn_cell parameter\n",
    "```\n",
    "- a [prediction](https://pytorch.org/docs/stable/nn.html#linear) layer (hidden_size, output_size)\n",
    "\n",
    "```\n",
    "nn.Linear(in_size,out_size)\n",
    "```\n",
    "=> Complete the `init` function code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as f\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_char, hidden_size, output_size, n_layers=1,rnn_cell=nn.RNN):\n",
    "        \"\"\"\n",
    "        Create the network\n",
    "        \"\"\"\n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        self.n_char = n_char\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        #  (batch,chunk_len) -> (batch, chunk_len, hidden_size)  \n",
    "        self.embed = nn.Embedding(n_char,hidden_size)\n",
    "        \n",
    "        # (batch, chunk_len, hidden_size)  -> (batch, chunk_len, hidden_size)  \n",
    "        self.rnn = rnn_cell(hidden_size,hidden_size)\n",
    "        \n",
    "        #(batch, chunk_len, hidden_size) -> (batch, chunk_len, output_size)  \n",
    "        self.predict = nn.Linear(hidden_size,output_size)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        batched forward: input is (batch > 1,chunk_len)\n",
    "        \"\"\"\n",
    "        input = self.embed(input)\n",
    "        output,_  = self.rnn(input)\n",
    "        output = self.predict(f.tanh(output))\n",
    "        return output\n",
    "    \n",
    "    def forward_seq(self, input,hidden=None):\n",
    "        \"\"\"\n",
    "        not batched forward: input is  (1,chunk_len)\n",
    "        \"\"\"\n",
    "        input = self.embed(input)\n",
    "        output,hidden  = self.rnn(input.unsqueeze(0),hidden)\n",
    "        output = self.predict(f.tanh(output))\n",
    "        return output,hidden\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Text generation function\n",
    "\n",
    "Sample text from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model,prime_str='A', predict_len=100, temperature=0.8):\n",
    "    prime_input = char_tensor(prime_str).squeeze(0)\n",
    "    hidden = None\n",
    "    predicted = prime_str+\"\"\n",
    "    # Use priming string to \"build up\" hidden state\n",
    "\n",
    "    for p in range(len(prime_str)-1):\n",
    "        _,hidden = model.forward_seq(prime_input[p].unsqueeze(0),hidden)\n",
    "            \n",
    "    #print(hidden.size())\n",
    "    for p in range(predict_len):\n",
    "        output, hidden = model.forward_seq(prime_input[-1].unsqueeze(0), hidden)\n",
    "                # Sample from the network as a multinomial distribution\n",
    "        output_dist = output.data.view(-1).div(temperature).exp()\n",
    "        #print(output_dist)\n",
    "        top_i = torch.multinomial(output_dist, 1)[0]\n",
    "        #print(top_i)\n",
    "        # Add predicted character to string and use as next input\n",
    "        predicted_char = all_characters[top_i]\n",
    "        predicted += predicted_char\n",
    "        prime_input = torch.cat([prime_input,char_tensor(predicted_char).squeeze(0)])\n",
    "\n",
    "    return predicted\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Training loop for net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0m 2s (100 0%) 2.5365]\n",
      "Whtere, INThe, be than f here, m f cest is inotht ed I my there fr thal are y.\n",
      "Shea whlis bories hef w \n",
      "\n",
      "[0m 3s (200 1%) 2.5097]\n",
      "Whivey kerlit cke f s owilousive m athal cend mef tes outh wio my whane le, sthathan t ounge winan yon \n",
      "\n",
      "[0m 4s (300 1%) 2.5139]\n",
      "Whthe for hew tre ie je hichest, w f I thater d hinlend t hourow d t carthe burd yod har wowito frimyo \n",
      "\n",
      "[0m 5s (400 2%) 2.4861]\n",
      "Wheng.\n",
      "Whand bow\n",
      "Why y bou,\n",
      "I k t bel wanifoulve this vest wasthathence d ie n d tere he omille an hit \n",
      "\n",
      "[0m 6s (500 2%) 2.4811]\n",
      "Wh If whth ithay g wer qucusl and t in r, ss torenencod chousthee t mose t.\n",
      "WINUNENG besoresp sio hith \n",
      "\n",
      "[0m 7s (600 3%) 2.4930]\n",
      "Wh, mere hathele ne Houmy fazeeve t wome at\n",
      "Fis tere iso te t,\n",
      "Thaisofall in\n",
      "The y wlis; t,\n",
      "Hat ard,\n",
      "H \n",
      "\n",
      "[0m 8s (700 3%) 2.4391]\n",
      "Whourses, w wick windamowe CIZAnd is ass o,\n",
      "Nofrs fos hepeanotrd inl the thengh andothe ghut g ns fofa \n",
      "\n",
      "[0m 9s (800 4%) 2.4792]\n",
      "Whe gour e cour.\n",
      "\n",
      "QUS:\n",
      "LI:\n",
      "T:\n",
      "To t\n",
      "The KEO:\n",
      "Therid sty\n",
      "Nos ge-t TUS:\n",
      "The meshinere g rs wedon's VIfore \n",
      "\n",
      "[0m 10s (900 4%) 2.4794]\n",
      "Why me l thenthe po.\n",
      "\n",
      "And t uthothincest thoulll ardindothe whin, to, h th:\n",
      "\n",
      "\n",
      "Thounth mere tanar fow i \n",
      "\n",
      "[0m 12s (1000 5%) 2.5025]\n",
      "Wheatours d.\n",
      "My t ced sthanthe? my fthre haind t hess hand my,\n",
      "Desthailis oul.\n",
      "\n",
      "LANoong, he brs oofow, \n",
      "\n",
      "[0m 13s (1100 5%) 2.4938]\n",
      "Wh me athang hal why, noughaleras tha fill thatheted ale owire thesengalis:\n",
      "INTooure w ce fistoowe tho \n",
      "\n",
      "[0m 14s (1200 6%) 2.5364]\n",
      "Whe st f s fomeeant t,\n",
      "\n",
      "I:\n",
      "An t is anod ft th an, tan ay themechet achend I thige lth h big;\n",
      "\n",
      "He ll ho \n",
      "\n",
      "[0m 15s (1300 6%) 2.4572]\n",
      "Whes May a t, t h. tendonte an ire s fathe y ce h my my st me h nd s be ne y LULI ant ane owithe meace \n",
      "\n",
      "[0m 16s (1400 7%) 2.4338]\n",
      "Who wernd ous turoie to aithithakndaw hied ard mall the l s, totirs hacolle orgoullle cou wenceanche m \n",
      "\n",
      "[0m 17s (1500 7%) 2.4944]\n",
      "Whe be th toumsta wisthe, ther winerse ht llakeourer ngherantotheren me meveethil mound ou rsthes thal \n",
      "\n",
      "[0m 18s (1600 8%) 2.4952]\n",
      "Whe t ve blll u orourtholllithake vintongor our meagarsho tiner the way t thethug edshico ave inghe th \n",
      "\n",
      "[0m 19s (1700 8%) 2.4565]\n",
      "Who sthake nyow ano trel he what, buloud sime qud t pangl myonesk\n",
      "Torse p hef.\n",
      "Whe imbo ou my wives th \n",
      "\n",
      "[0m 20s (1800 9%) 2.4364]\n",
      "Wheas e, she Gr cloul tourfly incl Foul th stots, y. ad s;\n",
      "I isetheanothas thes.\n",
      "Whiersithoulery sowin \n",
      "\n",
      "[0m 21s (1900 9%) 2.4607]\n",
      "Whesceng te cknouredof m s, ystan t d what woan ou.\n",
      "An, ourd me tat uld mest wisthe thend bes t ir n l \n",
      "\n",
      "[0m 22s (2000 10%) 2.4170]\n",
      "Whit araveserenoe thike fen, shascengen\n",
      "Te.\n",
      "CHe I dss nge n th'the warvetothe t drt thes tisomale bant \n",
      "\n",
      "[0m 23s (2100 10%) 2.4380]\n",
      "Whe artimelenou, be be met dsithay oforaves senthe buthe Win marond t s llas the ilithaieaterr t ner's \n",
      "\n",
      "[0m 24s (2200 11%) 2.4532]\n",
      "Whe yor t is icers wat wirandis gr mer's, nigo whenghong, amathar\n",
      "Bur hende won ha anay tof.\n",
      "\n",
      "\n",
      "Wer ge  \n",
      "\n",
      "[0m 25s (2300 11%) 2.5063]\n",
      "Whane, windod imouin omyore swe CEd ge, be t, mand send be, hind reang duirar terto haitoungg cend sem \n",
      "\n",
      "[0m 27s (2400 12%) 2.4577]\n",
      "Where heat, ma w astha he s?\n",
      "\n",
      "Fin cte br thak im d ofowatrr thind tha n's! marouind hare s; th s manol \n",
      "\n",
      "[0m 28s (2500 12%) 2.4894]\n",
      "Whinoot thalene thed, he toorerlendy esthad r musis f a d ht, ke wisuf owon, watr ngallor wity!\n",
      "\n",
      "Fit n \n",
      "\n",
      "[0m 29s (2600 13%) 2.4359]\n",
      "Whegure wngun o de igsateanongn fe ome.\n",
      "\n",
      "Mal mand ms ime t y mer t ther mm?\n",
      "And wioubayour maco h brd  \n",
      "\n",
      "[0m 30s (2700 13%) 2.4674]\n",
      "Whe if n wind t of isical HARAngillly bll the s yoo ouberowhish nker\n",
      "O:\n",
      "Maf me'd at whonterust he I me \n",
      "\n",
      "[0m 31s (2800 14%) 2.5010]\n",
      "Whte HOf se e itom, sisorbullis th f higethe whe IR:\n",
      "TIXENEETom me torariroda, ee theneaven, il blly.\n",
      " \n",
      "\n",
      "[0m 32s (2900 14%) 2.4909]\n",
      "Whyous wave!\n",
      "TENTherir t theren stond bathist knghe y t?\n",
      "Whin thokin hanth, s Bull\n",
      "\n",
      "'sthe w outid he,  \n",
      "\n",
      "[0m 33s (3000 15%) 2.4654]\n",
      "Whur arin\n",
      "Herengory,\n",
      "\n",
      "\n",
      "\n",
      "MESotherur oreldind thatharofran my ag alf ncofors'sthar My,\n",
      "Yo tedes; n bepan \n",
      "\n",
      "[0m 34s (3100 15%) 2.4331]\n",
      "Whaldis it lee therth y ha,\n",
      "And, ange pp ouser goneeed I burnver s CAnchan joumas pr I le f by he, asp \n",
      "\n",
      "[0m 35s (3200 16%) 2.5027]\n",
      "Wh matondowang lof t ly fopr ourel ir w ntono be hingr win.\n",
      "QUK:\n",
      "We he mis t cico, f ed fithest hy wha \n",
      "\n",
      "[0m 36s (3300 16%) 2.4516]\n",
      "Whamprened presofriven foous t d hr houn s ort the d y the, h t whery he ic manan,\n",
      "IO:\n",
      "IVINGare mys fa \n",
      "\n",
      "[0m 37s (3400 17%) 2.4881]\n",
      "Whataitshis me; s terintour, atiter p ighabe he t measel by mu ss byor.\n",
      "\n",
      "Bus int tarichichatint r iso  \n",
      "\n",
      "[0m 38s (3500 17%) 2.4559]\n",
      "Wh h REO:\n",
      "\n",
      "Sande ms tawin wapespe cher, mace ano aives te ce s lkisethe ses t spal fomy, wisno, herld  \n",
      "\n",
      "[0m 39s (3600 18%) 2.4705]\n",
      "Whea theace shove ngak lilido nk a m,\n",
      "\n",
      "Scend ds g hend oud thesthyould f s.\n",
      "TI warefofowag fin m st of \n",
      "\n",
      "[0m 41s (3700 18%) 2.4364]\n",
      "Whesas ithay d:\n",
      "TIO, me an mous f won lasospawichof hangal mare fugh nore ho thatonere ou k Bauthe Gle \n",
      "\n",
      "[0m 42s (3800 19%) 2.4277]\n",
      "Whelle anan pller t ld n l;\n",
      "Taniave trgalladres thath I:\n",
      "\n",
      "ARDWinsthithe E:\n",
      "WIUKouloue bouthouthar fld  \n",
      "\n",
      "[0m 43s (3900 19%) 2.4427]\n",
      "Wh.\n",
      "Ther w e arsu lan, y\n",
      "ARKEMyo ised thate car witomy seas bay\n",
      "Y lid y RKank;\n",
      "\n",
      "Theerepe my ofe llonnd \n",
      "\n",
      "[0m 44s (4000 20%) 2.4630]\n",
      "Wha wa t, ind had w hemewhy ay\n",
      "Whaus anon!\n",
      "Thinfoklou mon ie basthak hathind hean ce wo, m s; e, foy,\n",
      " \n",
      "\n",
      "[0m 45s (4100 20%) 2.4663]\n",
      "Whane f had cour He mepe hor?\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "GR: wis Goo be n he myous, HAn wne?\n",
      "A:\n",
      "I be pry\n",
      "Forouen ithenearave  \n",
      "\n",
      "[0m 46s (4200 21%) 2.4463]\n",
      "Whean se.\n",
      "Th aleelyare hene. thee by hourove:\n",
      "An f aleanday t d aindid INCINRI oulem t hir s, ath sani \n",
      "\n",
      "[0m 47s (4300 21%) 2.4503]\n",
      "Whinfe ngof wo f.\n",
      "\n",
      "ILontimoulie t\n",
      "S:\n",
      "\n",
      "\n",
      "\n",
      "wn beank, e mor m th inot wh!\n",
      "\n",
      "Thove ave tris wepost be! he wa \n",
      "\n",
      "[0m 48s (4400 22%) 2.4957]\n",
      "Wh's y ge, fowifort thine a tipodrit seweny h be t thire acer'swor t h mithol.\n",
      "S:\n",
      "A:\n",
      "CEleadallaseile i \n",
      "\n",
      "[0m 49s (4500 22%) 2.4849]\n",
      "Whereare, thin Din r dooor bes mallers giepetorisinn g nso t orupr youlveed abllayopouse oouoron s aro \n",
      "\n",
      "[0m 50s (4600 23%) 2.4671]\n",
      "Where ther thir ame the t IAs se yorel tese ad nt beaneced\n",
      "She m\n",
      "\n",
      "ANo par;\n",
      "Beard s areathinowh d f We! \n",
      "\n",
      "[0m 51s (4700 23%) 2.5521]\n",
      "Whee arnd ad f sly lveren o by.\n",
      "GS:\n",
      "Biel de and athe foug allldoce or ave altind l itiad wana han g s  \n",
      "\n",
      "[0m 52s (4800 24%) 2.4625]\n",
      "Whee auplke por wllllin ialinonond TRI ourlpiso tht hin.\n",
      "\n",
      "Tr;\n",
      "Tou tordinorthanges mat t d masthe nd s  \n",
      "\n",
      "[0m 53s (4900 24%) 2.4429]\n",
      "Wh utr of ucepllous nd wan f y m.\n",
      "I tol lle macomis, and iler I;\n",
      "Thit:\n",
      "ANon s go f or ce y f t th thav \n",
      "\n",
      "[0m 54s (5000 25%) 2.4694]\n",
      "Wh HO:\n",
      "LLou thasurespore yoor mime merelleany m thatofor, INI:\n",
      "D:\n",
      "\n",
      "\n",
      "ARENGofe heve, wet f my f elle ber \n",
      "\n",
      "[0m 56s (5100 25%) 2.5092]\n",
      "Whthe r grrd t ared sso lomale, thearin d f ber;\n",
      "M:\n",
      "Thedenotho thoreangur stou che mar y ave he mishac \n",
      "\n",
      "[0m 57s (5200 26%) 2.4528]\n",
      "Wh aghay lle hen blathor'ss faveallsech, DUCooourod seavepour,\n",
      "Hathsas y.\n",
      "Wikindeme wades indonerr hio \n",
      "\n",
      "[0m 58s (5300 26%) 2.4309]\n",
      "Whan oproulonghe E ver hel tone me my.\n",
      "Mal asastle be:\n",
      "\n",
      "\n",
      "I t te fr ththe mear:\n",
      "Bubl, fes llo y or il y \n",
      "\n",
      "[0m 59s (5400 27%) 2.4863]\n",
      "Whe.\n",
      "IVIO:\n",
      "A:\n",
      "\n",
      "Bus tom bes tour CLLiove lld areer hary ind this t th t qur d I t ndomyowho mathan iton \n",
      "\n",
      "[1m 0s (5500 27%) 2.4688]\n",
      "Whome hay maleree th oowan ougourorno sthy aredin t m\n",
      "TIO:\n",
      "LI be me serel angr court y cosinknthardela \n",
      "\n",
      "[1m 1s (5600 28%) 2.3877]\n",
      "Whest bur 'spounsthind fin tond ome tif t ar t coueetoupllle bes wst berd itind ath bur matl\n",
      "\n",
      "That st  \n",
      "\n",
      "[1m 2s (5700 28%) 2.5106]\n",
      "Whoudint Myou t hame nges! are thenooure ighe thousond ck amy, sasbis haro s mstod LAste hethithies he \n",
      "\n",
      "[1m 3s (5800 28%) 2.4978]\n",
      "Whe:\n",
      "Bothes s f as t ce sponsus thithe nghour her fo.\n",
      "word nd unoveavertee tsimprt nes; sithas, ld ted \n",
      "\n",
      "[1m 4s (5900 29%) 2.4780]\n",
      "Wharond athobliou t lents vopairs:\n",
      "Qute todithiororime s bes se bere st sit, simat fe nd hol weng d me \n",
      "\n",
      "[1m 5s (6000 30%) 2.4648]\n",
      "Wh liss she t ctheantowil it raieme iowathesw twend aver it, t r t wit cexthelleetintheni.\n",
      "St the hin  \n",
      "\n",
      "[1m 6s (6100 30%) 2.5139]\n",
      "Whed, tharth nd, then at:\n",
      "qu or Bem had har there be mero akin ge ane' ouds chas tin be t, ous, fe, t  \n",
      "\n",
      "[1m 8s (6200 31%) 2.4692]\n",
      "Whisotrdeiord dere cr ye arth what cotepo higed t tucar, in mnoucoanat ais wed, anthemome me wencan bu \n",
      "\n",
      "[1m 9s (6300 31%) 2.4390]\n",
      "Whecoury on e.\n",
      "Whis co se t y r stis then is bes maithe gs II d wiothiges, stld my ce ou ave a h s te  \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1m 10s (6400 32%) 2.4816]\n",
      "Whithis lo.\n",
      "Anere it qukeerelusus t nd antullll tr tom.\n",
      "ONI d pr to at ar's ty I s f ccorand ay w ind. \n",
      "\n",
      "[1m 11s (6500 32%) 2.5104]\n",
      "Whaswan beat ache ass ma soochisei'd rd an pe!\n",
      "' it t in ollour t than ce bullolf thind, d, Pr, wind t \n",
      "\n",
      "[1m 12s (6600 33%) 2.4447]\n",
      "Whe we nd I unche IORELAn 'shrea gheroul as ur IIf MInd y spr IETh mur sthicer se il d s andond.\n",
      "TENGH \n",
      "\n",
      "[1m 13s (6700 33%) 2.4809]\n",
      "Whandomat's belouthid wieey son ry;\n",
      "Murtou we he pe:\n",
      "I sole ie kico bobl.\n",
      "Be t cay ghe WIUSTholouram a \n",
      "\n",
      "[1m 14s (6800 34%) 2.4757]\n",
      "Whe athises t mmy our winge le;\n",
      "To CO'l de, myo ond seser andernd,\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Buas h, s mouse g t se gherouth \n",
      "\n",
      "[1m 15s (6900 34%) 2.4084]\n",
      "Whean IIUCoun ausind, ngho,\n",
      "\n",
      "USININe bus ingr bulysidrkiso andstellorareliouthorithe hurechanthenghes  \n",
      "\n",
      "[1m 16s (7000 35%) 2.4664]\n",
      "Whid hes t I the f ie t th RINICESCEORo fay ands ou hithe t n t I'l nder hare ines iawitloue tthou,\n",
      "I  \n",
      "\n",
      "[1m 17s (7100 35%) 2.4139]\n",
      "Whaspourayod nddr il ncerd thid crore ous d se in onelld hutesor hathame the it che ff n yowithodeche  \n",
      "\n",
      "[1m 18s (7200 36%) 2.4569]\n",
      "Whor t thes ANI ai, hinesa he aur keraparcerees ather athar the d t se wetes anke f hanono he al hever \n",
      "\n",
      "[1m 19s (7300 36%) 2.4871]\n",
      "Whyouret heind touthe wiromenkeay me, bind thion inghithalleik berlis, ss corrour't t, t ispou,\n",
      "\n",
      "\n",
      "HARL \n",
      "\n",
      "[1m 20s (7400 37%) 2.5076]\n",
      "Whend oueame meres d n thand than w ser, ais y ase angsthave ourd, pangme, a ck wathoue:\n",
      "A:\n",
      "\n",
      "BEO, ap t \n",
      "\n",
      "[1m 21s (7500 37%) 2.4878]\n",
      "Wh be gen arerd VI mompe, antangangort bey!\n",
      "Who f cigt'd meird it ta t or me t se sthe w, m ewithe tha \n",
      "\n",
      "[1m 22s (7600 38%) 2.5335]\n",
      "Whigatare wd he owindidste CHof cuse prefr wit ind,\n",
      "My t of icrd sth ay myshe,\n",
      "\n",
      "Fovous, matho, t,\n",
      "AROf \n",
      "\n",
      "[1m 24s (7700 38%) 2.5021]\n",
      "Where, gr bence the my s pandy he II wheve wheathengle to t ar s:\n",
      "Thean is, ar thive walle.\n",
      "NSind\n",
      "T:\n",
      "A \n",
      "\n",
      "[1m 25s (7800 39%) 2.4558]\n",
      "Whakinere tot my ve se,\n",
      "An moy avee, o masoo'd ano d keaites t ngoo bl;\n",
      "TIfal.\n",
      "\n",
      "RINCHouno hen f al,\n",
      "Th \n",
      "\n",
      "[1m 26s (7900 39%) 2.4547]\n",
      "Whe sindor se,\n",
      "LAK:\n",
      "Ayomeanominf oninto o s gr wing te, cambe'th s achiou mat ie moove, dro ef winsthe \n",
      "\n",
      "[1m 27s (8000 40%) 2.3904]\n",
      "What thioucanthe winthomeatouthathof am, e t g t.\n",
      "MEx hast panou thy g thinghane, he ally te:\n",
      "GRI e it \n",
      "\n",
      "[1m 28s (8100 40%) 2.4231]\n",
      "Wheondshare as merothairith ofomst e\n",
      "\n",
      "IOLO:\n",
      "I tyor m me s hin anghe could te s thend No aistharllay be \n",
      "\n",
      "[1m 29s (8200 41%) 2.4636]\n",
      "Whotheaname I ishe Bow ckecer ghithest th t f gendin monge wey a agh INoume'thed SThes, me t anorer d  \n",
      "\n",
      "[1m 30s (8300 41%) 2.4821]\n",
      "Whin she d ow.\n",
      "I arend wis mef oveadexpe monkndsesou akis woveanthowiso st indile, s.\n",
      "\n",
      "\n",
      "RICYoncesund t \n",
      "\n",
      "[1m 31s (8400 42%) 2.5021]\n",
      "Whon mevende RIO:\n",
      "Mar.\n",
      "HARUS:\n",
      "Be t tithow n kesth my wdt at o wethar t ishithatrerimeait hander; t pic \n",
      "\n",
      "[1m 32s (8500 42%) 2.4310]\n",
      "Whe s m nde st rt pear at ha agritr sut l yomere.\n",
      "Deaksibulesthoulds s say lu llo w as an andr waveas  \n",
      "\n",
      "[1m 33s (8600 43%) 2.4640]\n",
      "Whome had bls t ndou t wer'TIOureroe t soure med g pinyond wowicke,\n",
      "Brithe, tor d\n",
      "SENINI nd thik, heru \n",
      "\n",
      "[1m 34s (8700 43%) 2.4422]\n",
      "Whep IOUCEd,\n",
      "Br, s hondand, in phinon de usthakndoree RI I sher shaint myon deanere man, an\n",
      "Th at moul \n",
      "\n",
      "[1m 35s (8800 44%) 2.4954]\n",
      "Whinous t\n",
      "Me fore bre mallengrou d atour\n",
      "Anete ind, cout tht Lowand anghat ar m thevermy an tomy mand  \n",
      "\n",
      "[1m 36s (8900 44%) 2.4052]\n",
      "Whares haindou wethacor man mouse mane,\n",
      "\n",
      "Cowenan hes myoouke mavithet\n",
      "Foferowishe\n",
      "\n",
      "I:\n",
      "D:\n",
      "QUKusent me,  \n",
      "\n",
      "[1m 37s (9000 45%) 2.4304]\n",
      "Whemathe SI'sis se borthinyove vet:\n",
      "\n",
      "\n",
      "Wimery bblince thathofat besethand lene bous al ERDe s thy ono h \n",
      "\n",
      "[1m 38s (9100 45%) 2.4849]\n",
      "Whenis be langred ndstor ph wite y ipiss ithoue on anthe wind thend e coutererinju t s the con ganoton \n",
      "\n",
      "[1m 40s (9200 46%) 2.4483]\n",
      "Whofithedire wrd sthale be, IOfan d\n",
      "\n",
      "The dal.\n",
      "IND:\n",
      "\n",
      "An ar byedel\n",
      "\n",
      "I:\n",
      "Sero mp'ead blisars he atoftheou  \n",
      "\n",
      "[1m 41s (9300 46%) 2.4516]\n",
      "Whull f by nind t ongouthenthece t ho a ig f y e TI the llin ay lll an.\n",
      "Ka as leefe hive themus t g ti \n",
      "\n",
      "[1m 42s (9400 47%) 2.4804]\n",
      "Whar n mer l be owot telor as; thatheer and yofo derd s t, theroucce wit my tre n wowhend o pat g nest \n",
      "\n",
      "[1m 43s (9500 47%) 2.4630]\n",
      "Whe wa tonthamy thie th ar souer heryond tre IORTh ou s ad wh lorerine f fthe\n",
      "\n",
      "Whoroousthe w thave t b \n",
      "\n",
      "[1m 44s (9600 48%) 2.4339]\n",
      "Whome:\n",
      "FONDWhe d mefu ed:\n",
      "To bouby anthe thoud tld ne myof my my te yomer tes and shice shigarouro the \n",
      "\n",
      "[1m 45s (9700 48%) 2.4784]\n",
      "Whinon w cheathas wen t e inst,\n",
      "\n",
      "Men f prathery m I bes che t cqul mart thelllit hie thes theve-be he  \n",
      "\n",
      "[1m 46s (9800 49%) 2.5126]\n",
      "Whilain I s me.\n",
      "Focece ando, chall t INe yor meime w,-\n",
      "\n",
      "\n",
      "D:\n",
      "\n",
      "Ane llo s ingears for h fout m ve INompot \n",
      "\n",
      "[1m 47s (9900 49%) 2.4241]\n",
      "Whollour's tavet wamet blicchif beleser asth y bloure he ire.\n",
      "Thy ar obuspethth hin m?\n",
      "Whend\n",
      "PE hads.\n",
      " \n",
      "\n",
      "[1m 48s (10000 50%) 2.5393]\n",
      "Whend han not tr ar bo STHithe\n",
      "Wistoulerd tis!\n",
      "Whem garoke my, tithe t y ad s nd orere INTh ancerem s  \n",
      "\n",
      "[1m 49s (10100 50%) 2.4214]\n",
      "Whe t fot mavitwnghe's.\n",
      "ICANAlast fis LIVinot pe athe tr thepon ald, be o ththe the gom t ins br hed a \n",
      "\n",
      "[1m 51s (10200 51%) 2.4700]\n",
      "Wha, y.\n",
      "S:\n",
      "I anghange and ere ishat s, t hime hamie aigrmiradird im mor fove O:\n",
      "\n",
      "NCave ty y y l bere,  \n",
      "\n",
      "[1m 52s (10300 51%) 2.4345]\n",
      "Whentave myoual CUFousou, touchase-hme?\n",
      "Astout reat nd che\n",
      "Goutet, fins e yolar thaloitowance is y ind \n",
      "\n",
      "[1m 53s (10400 52%) 2.5222]\n",
      "Whalond s od'd I pougo t:\n",
      "IENoou.\n",
      "\n",
      "T:\n",
      "\n",
      "An wand g therds,\n",
      "MAs mfores t cenour wintharvis ourell; wimish \n",
      "\n",
      "[1m 54s (10500 52%) 2.5029]\n",
      "Wheabe, anissillif w IO:\n",
      "Cowithestha s it then,\n",
      "\n",
      "IA:\n",
      "\n",
      "Tore mpare at wisieat thild,\n",
      "Whe an marlos cere  \n",
      "\n",
      "[1m 55s (10600 53%) 2.4269]\n",
      "Whereerd.\n",
      "Tie t hrver breagnove Wher tharo br t d storay,\n",
      "\n",
      "I se withe toutowik ty athind HAUS:\n",
      "INI cal \n",
      "\n",
      "[1m 56s (10700 53%) 2.4665]\n",
      "Whe strif iledshatherds, s tangen, cousepe an t 's hthe des s, thevit t le w, outhear,\n",
      "\n",
      "\n",
      "Le hergras it \n",
      "\n",
      "[1m 57s (10800 54%) 2.4307]\n",
      "Wht anoteritont wly, feyo chun th whisthe y se f n, be ke SA:\n",
      "ININUS:\n",
      "TI t ot t omy Bu s fame--blongou \n",
      "\n",
      "[1m 58s (10900 54%) 2.5192]\n",
      "Wh this w.\n",
      "BAll ther shis llind, gg moulis wis ou t ave se ore itin wirat for\n",
      "Angho ng mer afowingeroo \n",
      "\n",
      "[1m 59s (11000 55%) 2.4862]\n",
      "Whandousthen the bell be?\n",
      "Southy,\n",
      "WAnore bld t-ses fome he ouch t ps it out che thind\n",
      "Hand oficail y a \n",
      "\n",
      "[2m 0s (11100 55%) 2.4659]\n",
      "Wheer dere cont,\n",
      "SThincr.\n",
      "Thiculourrs finourethint my arst ur wigortanlly s h belelar thetherend 'd lo \n",
      "\n",
      "[2m 1s (11200 56%) 2.5052]\n",
      "Whe mpelin owouingrd, mathe mot feavenore en ar t bu, thous\n",
      "\n",
      "Whiniterd.\n",
      "ANEEmall, th peea t tse t, me' \n",
      "\n",
      "[2m 2s (11300 56%) 2.5349]\n",
      "Whand rendomy t t s m'd cit o?\n",
      "ANG t, heathathenomed sthared heathano is g\n",
      "MEOLOLO:\n",
      "\n",
      "Wheinpleain! se t \n",
      "\n",
      "[2m 3s (11400 56%) 2.5436]\n",
      "Whoucay, holore coman he,\n",
      "BUpo f ce whar bun s is,\n",
      "\n",
      "Anoonell d Burea hut dsaner t yomarecrevearoutomat \n",
      "\n",
      "[2m 4s (11500 57%) 2.4181]\n",
      "Whe\n",
      "HINThavelangher atin\n",
      "The wit, he matareres s he he hend kis aw s VI fe it, in ilf dasin,\n",
      "ARD:\n",
      "OLI' \n",
      "\n",
      "[2m 5s (11600 57%) 2.5063]\n",
      "Whin ie be w sthad I ast:\n",
      "IUS: tew shir an.\n",
      "\n",
      "SThe, d.\n",
      "\n",
      "UClima g--ticearen s\n",
      "be at, thaspouran arthalup \n",
      "\n",
      "[2m 6s (11700 58%) 2.5030]\n",
      "Wh whay.\n",
      "I monghay akicowhinooselangur ce ing t y.\n",
      "Honoure she theetr the by theas.\n",
      "Thapoprtore n teav \n",
      "\n",
      "[2m 7s (11800 59%) 2.5556]\n",
      "Whito whes, w.\n",
      "IIONI herr,\n",
      "ARUCLIOUSA:\n",
      "By ther;\n",
      "LLOKatharlyoury t hepodur d th Gouchen ld.\n",
      "Thil e thou \n",
      "\n",
      "[2m 9s (11900 59%) 2.4322]\n",
      "Wh ce t s be d s sprrd an at te.\n",
      "ARULLLouroust gis pearfoust aimouto be betond p fors ang me.\n",
      "The wil  \n",
      "\n",
      "[2m 10s (12000 60%) 2.4180]\n",
      "Whatr stind heale hy! hend thaind aisthe hian,\n",
      "Aheath hie,\n",
      "\n",
      "HE pe cerealof the, the m t hanousthe;\n",
      "\n",
      "\n",
      "C \n",
      "\n",
      "[2m 11s (12100 60%) 2.4524]\n",
      "Whes t thesthentho, t f s t.\n",
      "\n",
      "Sisld pe hin' le he, l ou th catist mpe bs\n",
      "An hathearther we har ome t s \n",
      "\n",
      "[2m 12s (12200 61%) 2.4489]\n",
      "When:\n",
      "PE he Tou nobend theedarche.\n",
      "He, find t w.\n",
      "ARim, l atin her n, we,\n",
      "Wakin.\n",
      "\n",
      "\n",
      "Cl thenglarllatr ay  \n",
      "\n",
      "[2m 13s (12300 61%) 2.4653]\n",
      "Whe ghe d cre med s spper o ceare ast th s contore frd het tece\n",
      "Myomathende ouray s\n",
      "LOrey haue mus fow \n",
      "\n",
      "[2m 14s (12400 62%) 2.4432]\n",
      "Whourout thel averd.\n",
      "Theaswe f borthely anghenver d cenomy hrt buronour sane the icer I's pporens t t  \n",
      "\n",
      "[2m 15s (12500 62%) 2.4974]\n",
      "Whang tong thite therant athesoman a ban, ave!\n",
      "Mysile t athonce tr;\n",
      "\n",
      "Wo houke\n",
      "M:\n",
      "\n",
      "ARYourit featy blo w \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2m 16s (12600 63%) 2.4697]\n",
      "Whe t he fothe ffe ace of y ayongnd mearthe me ke allo has I trepowefr br te be cis thes, calld serth  \n",
      "\n",
      "[2m 17s (12700 63%) 2.5008]\n",
      "Whed nceliknd he his bom ben s w, here thed dd f ld hatou e ckne melim:\n",
      "Bupl t s tourelatye\n",
      "KI whis, m \n",
      "\n",
      "[2m 18s (12800 64%) 2.4889]\n",
      "Whine thatondere oune o simow'd, p ceer.\n",
      "Win, silduchokispu helild, t ck atidinothiruclorsprn Silyo'd  \n",
      "\n",
      "[2m 19s (12900 64%) 2.4542]\n",
      "Who t gr, impouringe per peend ansind the is fonthif at t g\n",
      "Alily d INGof bisit tere wil ave\n",
      "Y o aind  \n",
      "\n",
      "[2m 20s (13000 65%) 2.4331]\n",
      "Whe sitheararishe dy bences m ore-th;\n",
      "ANGLendor blesange oime s yollserit\n",
      "Whe tod llllearayord por ld  \n",
      "\n",
      "[2m 21s (13100 65%) 2.4455]\n",
      "Whal t s in:\n",
      "PAnd ierdst t e ss ERD erilve bll py,\n",
      "Mye g d.\n",
      "NGLULLAnthat d t?\n",
      "AS:\n",
      "CHes,\n",
      "IUCoe.\n",
      "Frseve  \n",
      "\n",
      "[2m 22s (13200 66%) 2.4576]\n",
      "Whe?\n",
      "Weshendo at ke had ase arame thathuthelero th m RKE ale is beayot ais I s mene he hise mmy t:\n",
      "TOr \n",
      "\n",
      "[2m 23s (13300 66%) 2.4941]\n",
      "Wherd s. ive ppind thothen an sthand LOM:\n",
      "ARIULe ar ncere bave t ndinowir rereer d mivengound t:\n",
      "Toug  \n",
      "\n",
      "[2m 24s (13400 67%) 2.5195]\n",
      "Whe then I ma' ord n y dory ks o anthe thand!\n",
      "Hes mene;\n",
      "S:\n",
      "No w t I t s aisthe s w owon t he the therd \n",
      "\n",
      "[2m 25s (13500 67%) 2.4652]\n",
      "Whed atleaul haten.\n",
      "Th to akello he'see HAnous; cr I m wh, cl thor penind arout LOMy yo t ito thermf l \n",
      "\n",
      "[2m 27s (13600 68%) 2.4608]\n",
      "Whe he keeseeave s t, thinon iotho ma oun sper fe, he wellsouraleewh l t angowieckenced dd theates, t  \n",
      "\n",
      "[2m 28s (13700 68%) 2.4350]\n",
      "Whanous\n",
      "\n",
      "Wh roorof il linamancousthomy andouralin t bed ind encr the f ke; be sere moo, IOnlatheasat,  \n",
      "\n",
      "[2m 29s (13800 69%) 2.4659]\n",
      "Whar tithindl my seathee'd bud nthe;\n",
      "TENGanomondeans, al beand mecare thesind ee d alinosureathathatou \n",
      "\n",
      "[2m 30s (13900 69%) 2.4797]\n",
      "Whourd y; s rs ty Cod ile te thelithe:\n",
      "O:\n",
      "ARINTHA:\n",
      "Pr lllitheit dik'Th,\n",
      "bonde yo f that y and, t y s o \n",
      "\n",
      "[2m 31s (14000 70%) 2.4863]\n",
      "Wherest nd beng rthed yo mereder.\n",
      "ERDUnd t t d th waure f he t s ot ar boouse ande meamy,\n",
      "ANG chsh she \n",
      "\n",
      "[2m 32s (14100 70%) 2.5144]\n",
      "Whe, mere?\n",
      "Yout m ts pald ast man be ousur, we, hu me, I ale f dor s fr?\n",
      "\n",
      "Thichint mes gaioparr tof rs \n",
      "\n",
      "[2m 33s (14200 71%) 2.5325]\n",
      "Whicendiche a bs Preng llle, pangeat s eathaspie andinoacofad, imon wa thavematof an ollealatut, hourm \n",
      "\n",
      "[2m 34s (14300 71%) 2.5124]\n",
      "Whid os\n",
      "H:\n",
      "Boyorethe whano w thad te mevide owaratingoupre s w tiche p by u t\n",
      "\n",
      "\n",
      "\n",
      "F e s by we be ho wou \n",
      "\n",
      "[2m 35s (14400 72%) 2.4594]\n",
      "Wher u aind t, pound wncenowou l in an marsil it animoun. in o direndeidind a withe wan ce wive\n",
      "\n",
      "Who t \n",
      "\n",
      "[2m 36s (14500 72%) 2.4647]\n",
      "Whalosin, yor w lacurcu owind hather win you,\n",
      "Ag tich he are as y ongullfove, ck orof athe, MIO:\n",
      "Bor h \n",
      "\n",
      "[2m 37s (14600 73%) 2.5230]\n",
      "Whaithe mapour llorsthe.\n",
      "Whe!\n",
      "\n",
      "\n",
      "ARor, ce frise, h s watond at f thimaps ut b m alf ld the,\n",
      "I wher quro \n",
      "\n",
      "[2m 38s (14700 73%) 2.3800]\n",
      "Whas ir lound\n",
      "\n",
      "Of har nd Whe n and YO:\n",
      "AR an and belse ill y ind whit, s cor h sk lend leapadis han we \n",
      "\n",
      "[2m 39s (14800 74%) 2.4680]\n",
      "Whare hther RINon, atess. ds: mblly t:\n",
      "O:\n",
      "The d tore rowins.\n",
      "\n",
      "AUn t y ellout, et in lepoul VIse ben me \n",
      "\n",
      "[2m 40s (14900 74%) 2.4356]\n",
      "Wherghe, thar\n",
      "Thed, A:\n",
      "Thesorere lathas buthasen I be ware foulecathes saremolkirind thes mes foumanow \n",
      "\n",
      "[2m 41s (15000 75%) 2.4534]\n",
      "Whared itheanorde il pe,\n",
      "INGad,\n",
      "Wha tho'dal thingowe techexfeate f R:\n",
      "Mat icedsu athiuril at aya arsee \n",
      "\n",
      "[2m 42s (15100 75%) 2.4636]\n",
      "Whis mand ghe an d ar char, w y t indin tr avorein g s he ce asthan cathoise engay ell mof er bernct l \n",
      "\n",
      "[2m 43s (15200 76%) 2.4611]\n",
      "Whorffr\n",
      "\n",
      "\n",
      "\n",
      "G of myes. thinowiter nimese?\n",
      "Se s.\n",
      "RERIthnt, g n y s.\n",
      "ICENGour;\n",
      "II g me t, hearsthin, weam \n",
      "\n",
      "[2m 44s (15300 76%) 2.4720]\n",
      "Whath woroushill arod rou at he nde this angel ar pingere llle a t asou whesst ar mbs--myoves gowacher \n",
      "\n",
      "[2m 45s (15400 77%) 2.4700]\n",
      "Whar'ses h, ainthe ate, w s mend o jun he, farist thetr m.\n",
      "And cath bodours d al.\n",
      "\n",
      "HI'd orefr.\n",
      "ESThis  \n",
      "\n",
      "[2m 46s (15500 77%) 2.4680]\n",
      "Whe and wierdre r tinorime thenthee inse merd ar t on arveand I ate merteoder youth vethen e sethoread \n",
      "\n",
      "[2m 48s (15600 78%) 2.5352]\n",
      "Whed me t areit he bothe urmowhe loro, anore is,-gher aloullle ing hererloth whond s an ut teanghe dou \n",
      "\n",
      "[2m 49s (15700 78%) 2.4455]\n",
      "Whe t ld th o fe gove weans the icout toun or ostheve t ait asshag thantheldossesoford ave, s than tho \n",
      "\n",
      "[2m 50s (15800 79%) 2.4524]\n",
      "Whe have mbe co ave,\n",
      "HI g d pe bedotefond houimere he, l f arichen I ouppprak t my d\n",
      "\n",
      "Frere hean, che  \n",
      "\n",
      "[2m 51s (15900 79%) 2.4350]\n",
      "Whit?\n",
      "BELO:\n",
      "Carthe f t me hirandou wethe ishe de s t t is sothy t allave hat t:\n",
      "A:\n",
      "Hed, therul the ban \n",
      "\n",
      "[2m 52s (16000 80%) 2.4566]\n",
      "Whinise heand y haner t w tapr aneangus y; cous hes nowamered hon ak d Thetheande ent,\n",
      "UESee us ce g c \n",
      "\n",
      "[2m 53s (16100 80%) 2.4747]\n",
      "Whanden and t y d at ing VI I d heryow himy waleassour mureris s w n.\n",
      "Thinormyous te t cet seanerand g \n",
      "\n",
      "[2m 54s (16200 81%) 2.4782]\n",
      "Whiser ouspo this wh\n",
      "VOnt.\n",
      "Wer d?\n",
      "Asthely at so bora\n",
      "Thes bea f sh arre us ous he, theathe a k iciauis \n",
      "\n",
      "[2m 55s (16300 81%) 2.3979]\n",
      "Whame f ick f y wardar thith pouthese wor brel faveth te n ld, omy y yofance.\n",
      "We s a,\n",
      "BRDuriler ve'so, \n",
      "\n",
      "[2m 56s (16400 82%) 2.4481]\n",
      "Whan knd jese s trengo!\n",
      "Tofod CHissteshe thell therke, be, sethatr the brds dit oul ISAlarbeen isthave \n",
      "\n",
      "[2m 57s (16500 82%) 2.4156]\n",
      "Whenes mave.\n",
      "\n",
      "Do s t hte winod.\n",
      "\n",
      "\n",
      "KI t sthesur\n",
      "TRLI: ther resw'd we I'trere h o apangto su aures mee.\n",
      " \n",
      "\n",
      "[2m 58s (16600 83%) 2.4659]\n",
      "Wh atr h my sthe mouls ane Be couru a te, l omes ul s the thanthtorer at?\n",
      "\n",
      "Whonge kess t t wio ocond m \n",
      "\n",
      "[2m 59s (16700 83%) 2.4435]\n",
      "Wheamy po.\n",
      "\n",
      "NNTRINatr my; bre d. aind myonyon:\n",
      "MOMammave wn to s se thers s n whed h hryo INond as twe \n",
      "\n",
      "[3m 0s (16800 84%) 2.4624]\n",
      "Whbeshy ch aimerla, f sthrneyof, aiso ig, w\n",
      "S:\n",
      "Foue are s win:\n",
      "\n",
      "NTowimeleneavestork s, bup ss senchay\n",
      " \n",
      "\n",
      "[3m 1s (16900 84%) 2.4926]\n",
      "Whe tuches t br thaved lilsu su bavisenef t s hest be bre!\n",
      "Me bescor,\n",
      "\n",
      "ENGRUTI t whor or t wife, fan h \n",
      "\n",
      "[3m 2s (17000 85%) 2.4076]\n",
      "Whaind cun thay p DIUpe ges mal dis u fa mo fres arthinor le t hetou g.\n",
      "\n",
      "Theaghases PThe s ave an pe y \n",
      "\n",
      "[3m 3s (17100 85%) 2.4694]\n",
      "Wheatourde schalinigolofr goner's per, thierthepes at ONoulthes be htht our ce hariosthe a ht naly nen \n",
      "\n",
      "[3m 4s (17200 86%) 2.4373]\n",
      "Wherg fer; sthis wet orir se arwiller m thad, wis s yequdonde hemad I be, t men hishatengr! m t t ilea \n",
      "\n",
      "[3m 5s (17300 86%) 2.4296]\n",
      "Whe mave se w'd, Thalil s\n",
      "\n",
      "MBE: co purd;\n",
      "Tor winome he tene the, y s mare le tore mea arine, gl it ghe \n",
      "\n",
      "[3m 6s (17400 87%) 2.4306]\n",
      "Whothe belin m ande t I'dithethy shea, t LIfarong che anche d s s wo bed se mounsto st my chen rst ce  \n",
      "\n",
      "[3m 8s (17500 87%) 2.5117]\n",
      "Wh, wofr ime him fry shto s ake leveenss mma yo boy NGovoo h y hyofuth s om t nthent;\n",
      "BETof,\n",
      "Wethere m \n",
      "\n",
      "[3m 9s (17600 88%) 2.4505]\n",
      "Whe thelinsoote wenont t fowhe mare?\n",
      "Hathen d culld s l orar.\n",
      "WAnd adonorent therrisenghake ppl.\n",
      "Ald d \n",
      "\n",
      "[3m 10s (17700 88%) 2.4665]\n",
      "Whe d e bo om d ttys lk.\n",
      "I s alins an thatsoruret aleethan\n",
      "\n",
      "Tard trstsththeit hig halerun tis tht I f  \n",
      "\n",
      "[3m 11s (17800 89%) 2.5004]\n",
      "Whe ofou s tare hinou s the hy aroug nd hyothare g hes s ous fourland I'd a\n",
      "I'semenoure y s u areas I  \n",
      "\n",
      "[3m 12s (17900 89%) 2.4598]\n",
      "Wh tis byaruf ig tollowone iso wontalaburane we aig a I than\n",
      "\n",
      "Looulit t my prily t f l tessundo bu wid \n",
      "\n",
      "[3m 13s (18000 90%) 2.4236]\n",
      "Whant f is he gas I Anouan f hathoulanouphir wsiceeso al stes!\n",
      "\n",
      "\n",
      "\n",
      "Samif by the? whounse, tharing bermu \n",
      "\n",
      "[3m 14s (18100 90%) 2.4655]\n",
      "Whordinou bly caistoo t sle be.\n",
      "Tha t w inicee e's d,\n",
      "Horenindir thogon wince, dad teall me MAngl antr \n",
      "\n",
      "[3m 15s (18200 91%) 2.4082]\n",
      "Whethis TEN:\n",
      "\n",
      "Thr.\n",
      "I s the, trerdorr n athimivatecinse t isoue Lal and ave y, chaverorenthe, wirerould \n",
      "\n",
      "[3m 16s (18300 91%) 2.4725]\n",
      "Whin ke y be wourdor s ithint apend tovee cear nd:\n",
      "The hak'dsare tigo pove alen is me h, nd jur ill th \n",
      "\n",
      "[3m 17s (18400 92%) 2.4736]\n",
      "Whed w t,\n",
      "Frr the wrin ces mosed sune gs m f tt,\n",
      "Fithet LLinile shaund t whakse ce hr beeng tlisou s.  \n",
      "\n",
      "[3m 18s (18500 92%) 2.4583]\n",
      "Wha awing t we hy f d hand sthisurirerthind ben moere me thor g hy s shee mer tord bldouaco Go g aid l \n",
      "\n",
      "[3m 19s (18600 93%) 2.4697]\n",
      "Whe wiese, s blame be ll Y isthe her s meisen llllme ls wid th hes\n",
      "My geserchiseane thesid waloms s am \n",
      "\n",
      "[3m 20s (18700 93%) 2.4809]\n",
      "Whane nof heas.\n",
      "Sintoor bousthanofoor se s s thenar y he ENG seshentun douglimminethe bus,\n",
      "An Yesthiga \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3m 21s (18800 94%) 2.4169]\n",
      "Whe kinad wing lerintoueind wiseave therd anourd he herng,\n",
      "Hesthe. wn weis wiss t t, m br w hal'?\n",
      "Tour \n",
      "\n",
      "[3m 22s (18900 94%) 2.4398]\n",
      "When mowou thand,\n",
      "Wesurd at e Bu atos ffeinell ourerswoutes athes\n",
      "Ay, thand hit.\n",
      "Anco n r an r, t gha  \n",
      "\n",
      "[3m 23s (19000 95%) 2.5367]\n",
      "Whed ore t d pre th, st whaccllld cond hinerond:\n",
      "Thareswoan, n:\n",
      "IULI we thathire ere, alld s he he g d \n",
      "\n",
      "[3m 24s (19100 95%) 2.4974]\n",
      "Whithis oo mathave meat my, s r t hourcor thar ld th t h, t wo, ant ardecoure, m n thar seaye be sth:\n",
      " \n",
      "\n",
      "[3m 25s (19200 96%) 2.4819]\n",
      "Who fof senthe; mon ncof he.\n",
      "Pakimeued fathod keromy al Gl eeathecheren ithis the pre ome ES:\n",
      "CERedera \n",
      "\n",
      "[3m 27s (19300 96%) 2.4883]\n",
      "Whes heinoun hord bure serom an antishicomas's be hint he, boue theave,\n",
      "\n",
      "AN s isthom t\n",
      "\n",
      "ARUCENICHe S:\n",
      " \n",
      "\n",
      "[3m 28s (19400 97%) 2.4590]\n",
      "Whond at ms, t t wed angnd; h.\n",
      "Pown y t cand pthind ghe art RET:\n",
      "Fase my;\n",
      "Hare tomet d th, h ng I he t \n",
      "\n",
      "[3m 29s (19500 97%) 2.4392]\n",
      "Whe pls yer:\n",
      "TENNANThend, ghe o t avethin he, f tofe thablloulan:\n",
      "\n",
      "Why that forn as yourthearacom tar  \n",
      "\n",
      "[3m 30s (19600 98%) 2.4305]\n",
      "Whear the isen ste s y tho ouramyoreng stheake;\n",
      "D:\n",
      "Iss th\n",
      "ENINAy ss s s ake ir noustomens foukenanithe \n",
      "\n",
      "[3m 31s (19700 98%) 2.4204]\n",
      "Whe y burioure hthere.\n",
      "Fiad the whe\n",
      "\n",
      "Wif bere t p cel gha t f dove howins fe isunous, cugis totortre o \n",
      "\n",
      "[3m 32s (19800 99%) 2.5175]\n",
      "Whingn ooloont hay tor my ouril cothe sthanang.\n",
      "\n",
      "Ales s our brphis s.\n",
      "WBulelousthereronorevent she,\n",
      "Ma \n",
      "\n",
      "[3m 33s (19900 99%) 2.4657]\n",
      "Whe, le itenghe f whintarene pon w, tonimyigenge.\n",
      "\n",
      "\n",
      "Re ibs pe blle he hore hrter tus at f\n",
      "thin an ises \n",
      "\n",
      "[3m 34s (20000 100%) 2.5027]\n",
      "Whers mompe the s:\n",
      "ARWisowe souge deathisots,\n",
      "Thore, ithite e bld yovelourat ceroo,\n",
      "GRLENI thin the Go \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def time_since(since):\n",
    "    s = time.time() - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "###Parameters\n",
    "n_epochs = 20000\n",
    "print_every = 100\n",
    "plot_every = 10\n",
    "hidden_size = 100\n",
    "n_layers = 5\n",
    "lr = 0.005\n",
    "batch_size = 16\n",
    "chunk_len = 80\n",
    "\n",
    "####\n",
    "\n",
    "model = RNN(n_characters, hidden_size, n_characters, n_layers) #create model\n",
    "model_optimizer = torch.optim.Adam(model.parameters(), lr=lr) #create Adam optimizer\n",
    "criterion = nn.CrossEntropyLoss() #chose criterion\n",
    "\n",
    "start = time.time()\n",
    "all_losses = []\n",
    "loss_avg = 0\n",
    "\n",
    "\n",
    "def train(inp, target):\n",
    "    \"\"\"\n",
    "    Train sequence for one chunk:\n",
    "    \"\"\"\n",
    "    #reset gradients\n",
    "    model_optimizer.zero_grad() \n",
    "    \n",
    "    # predict output\n",
    "    output = model(inp)\n",
    "    \n",
    "    #compute loss\n",
    "    loss =  criterion(output.view(batch_size*chunk_len,-1), target.view(-1)) \n",
    "\n",
    "    #compute gradients and backpropagate\n",
    "    loss.backward() \n",
    "    model_optimizer.step() \n",
    "\n",
    "    return loss.data.item() \n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    loss = train(*random_training_set(chunk_len,batch_size))  #train on one chunk \n",
    "    loss_avg += loss\n",
    "\n",
    "    if epoch % print_every == 0:\n",
    "        print('[%s (%d %d%%) %.4f]' % (time_since(start), epoch, epoch / n_epochs * 100, loss))\n",
    "        print(generate(model,'Wh', 100), '\\n')\n",
    "       \n",
    "\n",
    "\n",
    "    if epoch % plot_every == 0:\n",
    "        all_losses.append(loss_avg / plot_every)\n",
    "        loss_avg = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-17 15:58:03,888 : INFO : Failed to extract font properties from /usr/share/fonts/truetype/noto/NotoColorEmoji.ttf: In FT2Font: Can not load face.  Unknown file format.\n",
      "2022-02-17 15:58:04,664 : INFO : generated new fontManager\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f885a34e5f8>]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAkmUlEQVR4nO3deXxU5dn/8c+VEHYElIAISlAEtShbXFHrirjUvVpr3VofbJ/ap1Zt3W3d6laXn9Vq3bXauuGCuAACihtLwIR9k0U2IeyEJev1+2PODDPJJJlgJuHQ7/v1yiuTM2fOuebMzHfu3Oc+55i7IyIi4ZfR2AWIiEj9UKCLiOwiFOgiIrsIBbqIyC5CgS4isoto0lgr7tChg+fk5DTW6kVEQmny5Mmr3T072X2NFug5OTnk5eU11upFRELJzBZXd5+6XEREdhEKdBGRXYQCXURkF6FAFxHZRSjQRUR2EQp0EZFdhAJdRGQXEbpAn7tyEw+PnMPqouLGLkVEZKcSukCft7KIx8bMZ+3mksYuRURkpxK6QI/SdTlERBKFLtDNGrsCEZGdU+gCPcpRE11EJF7oAl0NdBGR5EIX6FHqQxcRSRS6QI/2oSvQRUQShS7Q1ekiIpJcrYFuZs3NbKKZFZjZDDO7o5r5LjCzmcE8/67/UhNpp6iISKJUrlhUDJzg7kVmlgV8YWYfufv46Axmtj9wEzDQ3deZWcc01athiyIi1ag10N3dgaLgz6zgp3Lz+H+AJ9x9XfCYVfVZZPK60r0GEZFwSakP3cwyzSwfWAWMcvcJlWbpCfQ0sy/NbLyZDa5mOUPMLM/M8goLC3eoYDXQRUSSSynQ3b3c3fsCXYHDzKx3pVmaAPsDxwEXAc+YWbsky3na3XPdPTc7O+lFq0VEZAfVaZSLu68HxgKVW+BLgWHuXuruC4G5RAK+3pk60UVEkkpllEt2tLVtZi2Ak4HZlWZ7l0jrHDPrQKQLZkE91lmF+tBFRBKlMsqlM/CSmWUS+QJ4w92Hm9mdQJ67DwNGAIPMbCZQDvzR3deko2C1z0VEkktllMtUoF+S6bfH3Xbg2uCnQWgcuohIotAdKapD/0VEkgttoIuISKLQBXqUGugiIolCF+im3aIiIkmFLtCjXJ3oIiIJwhfoaqCLiCQVvkAPqH0uIpIodIGuBrqISHKhC/QodaGLiCQKXaDr5FwiIsmFLtC3UxNdRCRe6AJd7XMRkeRCF+hR6kMXEUkUukCPnZyrccsQEdnphC/Q1ekiIpJU6AI9Sl0uIiKJQhfoGrUoIpJc6AI9SifnEhFJFLpAVwNdRCS50AV6lNrnIiKJwhfoaqKLiCQVvkAPqAtdRCRR6AJd49BFRJKrNdDNrLmZTTSzAjObYWZ31DDveWbmZpZbv2VW5epFFxFJ0CSFeYqBE9y9yMyygC/M7CN3Hx8/k5m1AX4PTEhDnXHrCW4oz0VEEtTaQveIouDPrOAnWZzeBdwPbKu/8qpSh4uISHIp9aGbWaaZ5QOrgFHuPqHS/f2Bvd39g1qWM8TM8swsr7CwcEdrBtRAFxGpLKVAd/dyd+8LdAUOM7Pe0fvMLAN4GLguheU87e657p6bnZ29QwXrikUiIsnVaZSLu68HxgKD4ya3AXoDn5rZIuAIYFi6d4xq2KKISKJURrlkm1m74HYL4GRgdvR+d9/g7h3cPcfdc4DxwJnunpeOgtVAFxFJLpUWemdgrJlNBSYR6UMfbmZ3mtmZ6S2vehq2KCKSqNZhi+4+FeiXZPrt1cx/3A8vq3pqoIuIJBe6I0Wj1IcuIpIodIGuPnQRkeRCF+hRaqCLiCQKYaCriS4ikkwIAz1Cl6ATEUkUukCP9qErzkVEEoUv0Bu7ABGRnVToAj1GTXQRkQShC3SdnEtEJLnQBXqUDv0XEUkUukBX+1xEJLnQBXqURi2KiCQKXaCrC11EJLnQBXqUWugiIolCF+imXnQRkaRCF+hRaqCLiCQKXaDHDv1Xn4uISILQBbqIiCQX2kBX+1xEJFHoAl3DFkVEkgtdoEepC11EJFHoAl3DFkVEkgtdoG+nJrqISLxaA93MmpvZRDMrMLMZZnZHknmuNbOZZjbVzEabWbf0lKs+dBGR6qTSQi8GTnD3PkBfYLCZHVFpnm+AXHc/BHgLeKBeq0xCfegiIolqDXSPKAr+zAp+vNI8Y919S/DneKBrvVYZRy10EZHkUupDN7NMM8sHVgGj3H1CDbP/CviomuUMMbM8M8srLCysc7Hx1EAXEUmUUqC7e7m79yXS8j7MzHonm8/MfgHkAg9Ws5yn3T3X3XOzs7N3qGCNchERSa5Oo1zcfT0wFhhc+T4zOwm4BTjT3Yvrpboaa0n3GkREwiWVUS7ZZtYuuN0COBmYXWmefsA/iYT5qjTUGbeuyG9dU1REJFGTFObpDLxkZplEvgDecPfhZnYnkOfuw4h0sbQG3rRI4n7n7memo2B1uIiIJFdroLv7VKBfkum3x90+qZ7rqpW6XEREEoXuSFENWxQRSS50gR6lBrqISKIQBrqa6CIiyYQw0CN0CToRkUShC3T1oYuIJBe6QBcRkeRCF+hqoIuIJBe6QI9SF7qISKLQBXpwJKoO/RcRqSR8gd7YBYiI7KRCF+hR6nIREUkUukDXsEURkeRCF+hRaqGLiCQKXaDrikUiIsmFLtCj1EAXEUkUukBXH7qISHKhC/QonZxLRCRRaANdREQShTbQ1T4XEUkUukBXH7qISHKhC/QYNdFFRBKELtB1ci4RkeTCF+iNXYCIyE6q1kA3s+ZmNtHMCsxshpndkWSeZmb2upnNN7MJZpaTlmrjaNSiiEiiVFroxcAJ7t4H6AsMNrMjKs3zK2Cdu/cAHgHur9cq42inqIhIcrUGukcUBX9mBT+V28dnAS8Ft98CTjRLb/SqgS4ikiilPnQzyzSzfGAVMMrdJ1SapQuwBMDdy4ANwB5JljPEzPLMLK+wsHCHCtbJuUREkksp0N293N37Al2Bw8ys946szN2fdvdcd8/Nzs7ekUXELesHPVxEZJdTp1Eu7r4eGAsMrnTXMmBvADNrArQF1tRDfVWoD11EJLlURrlkm1m74HYL4GRgdqXZhgGXBbfPB8Z4ms+epXHoIiKJmqQwT2fgJTPLJPIF8Ia7DzezO4E8dx8GPAf8y8zmA2uBn6WrYDXQRUSSqzXQ3X0q0C/J9Nvjbm8Dflq/pdVWV0OuTURk5xe6I0WjTXTluYhIotAFuoYtiogkF7pAj1Gfi4hIgtAFuoYtiogkF7pAj1L7XEQkUegCXQ10EZHkQhfoUepCFxFJFLpAT/NJHEVEQit0gR6V5jMLiIiETugCXe1zEZHkQhfoUWqfi4gkCl2gqwtdRCS50AV6lLrQRUQShS7Qo+dyUZ6LiCQKXaBrr6iISHLhC/SAhi2KiCQKXaBrp6iISHKhC3QREUkudIGuBrqISHKhC/QodaGLiCQKXaDr5FwiIsmFLtCjXCPRRUQShC7Q1T4XEUmu1kA3s73NbKyZzTSzGWb2+yTztDWz982sIJjnivSUu5360EVEEjVJYZ4y4Dp3n2JmbYDJZjbK3WfGzfNbYKa7/8TMsoE5Zvaqu5fUd8HRLnTluYhIolpb6O6+wt2nBLc3AbOALpVnA9pYZI9la2AtkS8CERFpIKm00GPMLAfoB0yodNfjwDBgOdAGuNDdK+qjwCo1RE/OpSa6iEiClHeKmllrYChwjbtvrHT3KUA+sBfQF3jczHZLsowhZpZnZnmFhYU7VLBGLYqIJJdSoJtZFpEwf9Xd304yyxXA2x4xH1gIHFB5Jnd/2t1z3T03Ozv7h9StYYsiIpWkMsrFgOeAWe7+cDWzfQecGMzfCegFLKivIkVEpHap9KEPBC4BpplZfjDtZmAfAHd/CrgLeNHMphEZKn6Du6+u/3K3Ux+6iEiiWgPd3b+gluN53H05MKi+iqpJZkaklIoKJbqISLzQHSmaEewVVZ6LiCQKYaBHfperz0VEJEHoAt3MyDB1uYiIVBa6QIdIP3qFWugiIglCGehmpi4XEZFKQhnomWbqchERqSScgZ5hlKflTDEiIuEVykDPMNSHLiJSSTgDXTtFRUSqCGWgZ5pRrj50EZEEoQx0tdBFRKoKZaBHRrk0dhUiIjuXUAZ6hunQfxGRysIZ6Bkahy4iUlkoAz0zQ0eKiohUFs5AN9Ppc0VEKglloJvOtigiUkUoAz1y6L8CXUQkXigDPUNnWxQRqSKUgZ6ZYbgCXUQkQSgDPUOH/ouIVBHOQM8wypXnIiIJQhnomRrlIiJSRa2BbmZ7m9lYM5tpZjPM7PfVzHecmeUH83xW/6Vup2uKiohU1SSFecqA69x9ipm1ASab2Sh3nxmdwczaAf8ABrv7d2bWMT3lxtanPnQRkUpqDXR3XwGsCG5vMrNZQBdgZtxsPwfedvfvgvlWpaHWmIkL16Zz8SIioVSnPnQzywH6ARMq3dUTaG9mn5rZZDO7tJ7qExGRFKUc6GbWGhgKXOPuGyvd3QQYAJwOnALcZmY9kyxjiJnlmVleYWHhDhe9b4dWABqLLiISJ6VAN7MsImH+qru/nWSWpcAId9/s7quBcUCfyjO5+9PunuvuudnZ2Ttc9HkDugJQUq6rXIiIRKUyysWA54BZ7v5wNbO9BxxtZk3MrCVwODCr/spM1DwrE4BtJQp0EZGoVEa5DAQuAaaZWX4w7WZgHwB3f8rdZ5nZx8BUoAJ41t2np6FeAFoEgb61tJy2ZKVrNSIioZLKKJcvAEthvgeBB+ujqNq0aBr5x2JraXlDrE5EJBRCeaRorIVeokAXEYkKZaA3j3W5lDVyJSIiO49QBnr7lk0BWLu5tJErERHZeYQy0Du0aQbA/7yc18iViIjsPEIZ6K2aZjZ2CSIiO51QBnrLpqmMthQR+e8SykBv2iSUZYuIpJWSUaSeXPlSHq9OWNzYZch/sdAH+l3DZ/JG3hKmLd3Q2KXsVNYUFbNNB141qE9mreSWd9J2gLRIrULfGf3cFwtjtxfdd3ojVpJo2fqtZBh0btuiUdY/4O5POLpHB1658vBGWb+INLzQt9DjfTpnFUvWbuH1Sd9x67vTGPzoONYUFSdc3ejj6StYvGZz0scXl5WzcuO2lNe3aVspkxevS3rfwPvGcOS9Y4DI9U8rXwN1TVExpSmcLTL37lGc9cSXKde0cVsp32+IPIcv5q9O+XHptGj1Zp4ZtyD295Tv1nHvh2k7d9sPVlxWzqZt6T3GobS8godGzmFjGtdTWl7B1KXrf9AyisvKeS9/WcqnqnZ33p6ylC0lkYP+tpWWc/2bBdz74SzWbi75QbXUZOm6LZz/5FdpXUd92LC1lJ63fsTn83b89OE1CW2gn35w5yrTLn9hEsc8MJYbhk7jlfHfMfv7TQy4+xP2u/lDpi+LdMn8+pUp/PjBTyktr+Dvo+extaScVRu38X7Bcq5/cyqH/3U0RcVlFCxZz4zlG3ht4ncJXwAbtpSyuTjyZr3mtXzOe/IrNm4rpbisvNpwH/ToOI66bwzuHvsZcPcnXPtGAdOXbSDnxg+Y8/2mhMeMnbOKouIyVheVULBkfcJ90cBO5si/juaIe0dXmV5cVs4Rfx3Ny18vqvax+UvW7/Cl/R4eNZefPzM+6X0/e3o893w4i6LiMpas3cK5//iKf45bQN6iyJWnKiqcO9+fSX7c83wzbwlff7umynMtS8MpkzdsLWXh6shrvHZzCb1u/ZiD/zIydt9H01bUaXk3Dp3KiBnf1zjP+wXL+fuY+Tw8ci4Aq4uKGTp5aY2P2VZazgMfz67Slfb5vEKKgvfkqk3bYtvxbyPncObjX8be+1EVFc7Y2atYXVQMQElZBTk3fsAr4xezaVspf3yzgCVrtwDw8Mi5/P61fLrf9CEAy9dv5aa3p1FaXsEX81ZXCfr8Jeu59o0Cbn9vBhBpZL01eSn/HLeAa9/Ij823ZO0W+t81ikWrExtX7+UvSwjlouIyZq3YfvmFpeu2sK008oXb89aPGDs7cnG0Zz9fSN7idfS/axQAvf88gpwbP2D9ltoDPvocor/nrtzEqk3bWLe5JOkXRHFZ+Q5dpP7db5bx2Oh5lJRV8MTY+XV+fCpC2+Xy0AV9+KAOH7Q/vJ7PvFVFsb/3v+UjAN4J3kDrt2xvKfX+84iky/jHxf3531en0L5lFpNvPZmCoN9++tINXPzcBNzhm9tOTugGKiuvYH6w3l63fczxvbL5+0X9gciHuku7SJfM6Nkr2b9jaxasLqJF0yZc8cKkhHX/9t9TePyifnw+bzWXPj+Rpy8ZwIoN25ixfAMPnB859by7sznJ+W2++nY1P38mcpGp29+bwfkDutKyaRM+n1dIm+ZZ3PH+DM7qsxd/eX8m/3NMd64b1IuyCmfZuq1MWrSWc/p1obisgns/nMWvj9uP/bJbA5Fz6Rx4+8e0ad6ETdsigfLUZ9/SrkUW+3dqw8Fd2tK0SUbsQ1HhzjEPjI3Vdf5TXzPimmN5a/ISnv9yIc9/uZBF953Oe/nL+ONbU2PzDf3NkeyzeysOvecTAI7rlc0+u7fkzrN6J32dAEbPWkmH1s3Ys21z1m4uYa92LdhcXMb0ZRsY9KM9eWLsfB4cMYd2LbNo2yKLxWu20LNTa+au3P4eGT51Ode+UUBJWQX7Zbfi1tMP4h+fzudvP+3D0nVbefbzBTxzaS5NMjMSgu21SUt4bdISvrzxBNZtLuH7DduocOe4Xh1p2iSDKd+t49o3CoDIOf3dndy7I8/ty29Xc9Wx+5Fh8OtXJvNt4WbG/fF4OrRpyivjF/OPT7+l3J2bTj0QgBUbtnLJcxMBePmXh3HdmwUUbipm0X2nM25u5D+0M/7+BX/+yUGcfkhnOrZpzqOj5/HY6HkAZLdpxu1nHATAre9OZ8SM7/l83mreDL5cDtizTex5lZVXcPM70/h0TiHTl21g2rINHNK1LT8/bB9GzlzJgG7teXDEHCDyX1jOjR8kvCafzilk1MyVZGbAL1+MHBT4Rt4S/jT4gNhz+f1r+RzefXdev+pIAH7zymQ+n7eaj685hv2yW3P0/ZH3z//7WV9Kyiq44sXI5+RXR3ePrWfJ2i2xL7hb353Ooxf2Zfn6bbz41SLO6NOZ/vu057HR83h41FxGXHMspzw6jkNz2jNpUfIG2aL7Tmf8gjUsXL2Z3G7tOfmRcZzXvysPXbD9kg/ujplRUlbB42PnU1pewZBj9qV9q8hR7RUVzjWv58fmb5KRnra0NdZVf3Jzcz0v74cd6Vn5DdOQmmZmJL3AxiFd2zK1lh20+befTN87Iy2Jti2y2LA18d/u/zuhB4+NqfoNvm92K360V1veL1ieMP3ecw/m6XELYq3MeAfs2YbZlVr/j17YN+HNlUxWplFanvy9Mf+eU5mxfGNKXUGXHNGNVyYsxh1evfJwLn628tULE10xMIcXvlxU63LjLbz3NMyMwk3FnP3Elyxbv7XG+efcPZhet35cp3VUJyvTaNuiKacfvCcvfV37CJdvbjuZfkErEiInmju73178Z+KSOq337rN7c0Hu3vzxrQLey19e5f5/XjKAq/41ucr0kX84loufnUDhpuI6rS9dzu3fhVN+tCf/959vOG9AV/494TsA2rfM4oHz+/C7/0xhW2nkc3b18T14PGjZ/miv3ZixvPKF05I7Zv8OfD5ve/fj5Ufl8OJXi1JeTuXHR919dm9eGb+Ypk0yav3MV3Zsz2xe/uVhdXpMlJlNdvfcpPcp0KWummQYZTvYNZMOTZtkUFIWjoudDOjWvtquOUldZobtcPfgzuCEAzry/OWH7tBjawr00PahA1hwlvZHLuzDXWdX/++31K+dKcyB0IQ5oDCvJ2EOc4AMq/USEzu23LQstYGMuOZYnvrFAM7p15VLjuhW47ynH1J1J6qISGP4zXH7pWW5oQ70np3aMLj3nrG/n788l39feTjDf3d0wgm8uu3Rkid+3j/296+O7s6vf7wfB3dpy7CrB3JOvy5Vlj3muh/XuO7z+neth2eQuj+e0osbTz2gQdfZWD69/rh6W9YvB3anV6c2ZKSnQZTg0Jz2ZNawovgPcU3zHdK1ba3r6tq+fo9v+O3x+9E0s/o4OOnAjnVeZp+921WZdlDn3WK3zzikM2/9+sg6LzdVFx22T50fE78TuC6GHLtvneY/sPOOrac2oR3lkswJB3SK3Z5x52AAFq7ezO7B+dOjbhh8QML5YB65sC/ZbZrxdDBWuuD2QbRtmcWwqweyZ9vmtG2RhTsccFtkR1put/Y8dEGf2F7ux8fMo+NuzTm+V0cq3Dn8r5Fhg8l2SMb73Qk9uOTIbjw+Zj7jF6xh7soiDtizDQfttRuFm4oTdsRccmS32BCtqPn3nMqlz0/kq2/XJEz/0+Be7JfduspOsT1aNeXN4AP02qQlDOzRgaN7dGC/mz+MzXPl0d3p0r4F+UvW02m35rRp1oTdWzeNHQF5br8uvP3NMq47uScPjZob6xM+8YCOzFqxkeVxwwybZ2XEdmglM/POUzjo9siIooI/D6LPHSO5YfAB5HRoRXabZhRuKubU3nvy0fTtQwBH/uFYBj0yrsqyrj6+B9cN6hkbXhfVomkGI/5wLGXlFeTe80lsNNPurZryuxN6cMf7M6ut75bTDmTy4nU0bZLBsIKqOx4h8l/izBUb+HHPjuwejGiI7ts58YCOPHf5obwyfjGPj5nPHwf1ok/XduyX3YoKh4+mr6Cs3GM7+ibeciJ7tGrGmqJijv/bp9x02oHc+m5kux+a056tpeVMX7aRe87pzUWH7sPmYKx34aZi7v1oNk2bZPDB1MjIr7/9tA/Xv1mQUOvsuwbH3sN3nPkjLjsqh03bSvnLsJkMOWY/rjmpJ29NXsqhOe3ptkcrVhcVs6BwMxc/O4GTD+rElpLy2Hvt/vMO5oah02jXMouR1xzLYX8dTbuWWeTfPoi3Ji/l+jcLeObSAVz63MTYZ2DePaeSlZkR2z6PB42sr286gTGzVyUcZTvn7sHc++FsXvxqEQd13o2ZcUMXK/+dzBc3HE/X9i25+oQe/OLZCbx+1RF0bNOc5eu3ctR9Y7ggtysPnN+nyn64d/53IJe9MJGJC9dy0oGduOec3rw2cQmPfDKXjm2a0aNja/5+UT8GBKOSJt58ImZGdptm9N+nPTcMncqwqweyoHAzV7w4iat+vC///Gz7MRhnHNI59rzTIdQ7Resq+uIlO6J0xIzvuepfk3nuslxOPLBTlfsBrnxpEp/MWsUjF/bhnH7Vt9AjB6aU8c6UZdwTHEDz+pAjOKz77pSWO9vKyllQuJmDu7SNtdQ+m1vIZc9P5Jj9O/CvX0WO7ux5y0eUlFfwxlVHclj33flk5kquDM4BH51vW2k5l78wkdvOOIjTH/si4flFn280HMffdCJ7tm1epd738pfxxNj5zF1ZxAPnH8IFuXsn3D9j+YbYsr++6QT23K15bFTJ9OUbuOKFSZzae0+uG9SLkx7+LPa4RfedzrCC5ezWvAnL12/j5nemATD0N0cxoFt7AFZu3MbazSUcGNdyA9hcXEZpeQXtWjZN+NAtuu/02HBJgJ/02Yv3C5bz5MX9OfXgzixcvZlrXs/n4sP2YeaKjVx/Si9aN6u+3fKLZyfQcbdmdN+jFXmL1/HXcw8GqHKU7/xVRazYsJUbh06LjaJ553+Pot8+7ass88J/fs2EhWtTPnL5J3//gmnLNjDltpNjXwpRS9dtYdm6rRy+7x5s2FpK4aZienRsnXQ5W0rKYl+QC+89LfbldsXAHM7u24U+e7dj8uK1tGzapMr2rsm6zSW0a5lFcVlF7Ash/rlF11vdjr5vC4soK3d6Ba3fmj6Hlc1fVUTH3Zrx0ye/Ztn6rQz9zVHs3qopr05YzNE9OjBzxcbYmPd4s+4cTItqTrM9f9Umuu3RiqzMyBDSc//xVY31lFc44xesYWCPDkBkCOLvXvuGy4/K4dCc3autPX/Jevp0bcvs7zeRlWnMX1XEwB4daNP8h13YvqadogkHuzTkz4ABA7yhnfTQp97thuHV3r949eZ6Xd/WkjLvdsNwf2X8olrnXbF+q3e7YbgPnbwkNm32io3+Vt72v0vKyv22d6f5ivVbky7jrbwl/tMnv4r9/dznC/yzOat80sI1/ssXJnpZeUW16/90zirvdsNwn7dyU5X7ysor/N4PZ/m8lRurva9w0zZ3d+99+8fe7Ybh/t2aqttyyMuT/N1vllZbQ3UmLlzjD42Y7SNnfB+b9n7BMr/r/Rm+cWuJPzF2Xo3PrT6VlJVXu/2jSsvKfWtJWcrLXLlxa8Lr/kOc/tg4P+b+Me7u/s6Upb5odVG9LLc205au96JtpSnN+/2Grb5yY83bMFXfb4h8bh4fM8/XFBV7cWntr09l5zzxhf/pzYJ6qachAHleTa7+V7XQN2wtZU1RMftmJ2/hNDYPDk4Is3krNzF2ziqGHJuenT4ilZWUVZCVaaH/7KSqphb6LtWHXpu2LSJHBe6sdoU35P6d2rB/p/Ts8BFJRtdH2K7WLWFme5vZWDObaWYzzOz3Ncx7qJmVmdn59VumiIjUJpUWehlwnbtPMbM2wGQzG+XuCcMDzCwTuB8YmYY6RUSkFrW20N19hbtPCW5vAmYBVQduw++AocCqJPeJiEia1anzycxygH7AhErTuwDnAE/WW2UiIlInKQe6mbUm0gK/xt0rj+p/FLjB3Ws8qYaZDTGzPDPLKyxMzwneRUT+W6U0bNHMsoDhwAh3fzjJ/QuB6BCNDsAWYIi7v1vdMhtj2KKISNj9oGGLFhlL9xwwK1mYA7h797j5XwSG1xTmIiJS/1IZ5TIQuASYZmb5wbSbgX0A3P2p9JQmIiJ10WhHippZIVD7JV6S6wDsHFdATrSz1gU7b22qq25UV93sinV1c/fsZHc0WqD/EGaWV10fUmPaWeuCnbc21VU3qqtu/tvq0jGzIiK7CAW6iMguIqyB/nRjF1CNnbUu2HlrU111o7rq5r+qrlD2oYuISFVhbaGLiEglCnQRkV1E6ALdzAab2Rwzm29mNzbwupOeG97M/mJmy8wsP/g5Le4xNwW1zjGzU9JY2yIzmxasPy+YtruZjTKzecHv9sF0M7PHgrqmmllarlprZr3itkm+mW00s2saY3uZ2fNmtsrMpsdNq/P2MbPLgvnnmdllaarrQTObHaz7HTNrF0zPMbOtcdvtqbjHDAhe//lB7T/oainV1FXn162+P6/V1PV6XE2LogdANvD2qi4bGvY9Vt216XbGHyAT+BbYF2gKFAAHNeD6OwP9g9ttgLnAQcBfgOuTzH9QUGMzoHtQe2aaalsEdKg07QHgxuD2jcD9we3TgI+InH/nCGBCA7123wPdGmN7AccC/YHpO7p9gN2BBcHv9sHt9mmoaxDQJLh9f1xdOfHzVVrOxKBWC2o/NQ111el1S8fnNVldle5/CLi9EbZXddnQoO+xsLXQDwPmu/sCdy8BXgPOaqiVe+rnho86C3jN3YvdfSEwn8hzaChnAS8Ft18Czo6b/rJHjAfamVnnNNdyIvCtu9d0dHDatpe7jwPWJllfXbbPKcAod1/r7uuAUcDg+q7L3Ue6e1nw53iga03LCGrbzd3HeyQVXo57LvVWVw2qe93q/fNaU11BK/sC4D81LSNN26u6bGjQ91jYAr0LsCTu76XUHKhpY1XPDX918K/T89F/q2jYeh0YaWaTzWxIMK2Tu68Ibn8PdGqEuqJ+RuIHrbG3F9R9+zTGdvslkZZcVHcz+8bMPjOzY4JpXYJaGqKuurxuDb29jgFWuvu8uGkNvr0qZUODvsfCFug7Bat6bvgngf2AvsAKIv/2NbSj3b0/cCrwWzM7Nv7OoCXSKGNUzawpcCbwZjBpZ9heCRpz+1THzG4hcgnIV4NJK4B93L0fcC3wbzPbrQFL2ulet0ouIrHR0ODbK0k2xDTEeyxsgb4M2Dvu767BtAZjkXPDDwVedfe3Adx9pbuXe+QCH8+wvZugwep192XB71XAO0ENK6NdKcHv6OUBG3o7ngpMcfeVQY2Nvr0Cdd0+DVafmV0OnAFcHAQBQZfGmuD2ZCL90z2DGuK7ZdJS1w68bg25vZoA5wKvx9XboNsrWTbQwO+xsAX6JGB/M+setPp+BgxrqJUHfXRVzg1fqf/5HCC6B34Y8DMza2Zm3YH9ieyMqe+6WlnkAt6YWSsiO9WmB+uP7iW/DHgvrq5Lgz3tRwAb4v4tTIeEllNjb684dd0+I4BBZtY+6G4YFEyrV2Y2GPgTcKa7b4mbnm2Ri7FjZvsS2T4Lgto2mtkRwXv00rjnUp911fV1a8jP60nAbHePdaU05PaqLhto6PfYD9mz2xg/RPYOzyXybXtLA6/7aCL/Mk0F8oOf04B/AdOC6cOAznGPuSWodQ4/cE96DXXtS2QEQQEwI7pdgD2A0cA84BNg92C6AU8EdU0DctO4zVoBa4C2cdMafHsR+UJZAZQS6Zf81Y5sHyJ92vODnyvSVNd8Iv2o0ffYU8G85wWvbz4wBfhJ3HJyiQTst8DjBEeB13NddX7d6vvzmqyuYPqLwK8rzduQ26u6bGjQ95gO/RcR2UWErctFRESqoUAXEdlFKNBFRHYRCnQRkV2EAl1EZBehQBcR2UUo0EVEdhH/H8g3rGKEE6sGAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(all_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try different temperatures\n",
    "\n",
    "Changing the distribution sharpness has an impact on character sampling:\n",
    "\n",
    "more or less probable things are sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THears ORo\n",
      "A:\n",
      "LOM:\n",
      "\n",
      "\n",
      "V:\n",
      "HULINAn t d fearer terse, NTy on ameariolisitoo al y aies bitte hsptoristo acoora-\n",
      "\n",
      "\n",
      "Be ast s betour scuized e touis hau e qustecerat ceanon y IOFof felamen'lld, thaitharvar my \n",
      "----\n",
      "Tho beathe y, bulomefagu ace har wis!'t d ff ane the hathare o t we nouromonn, mpe etus heake me hesth s toungant herdid hangro is te y\n",
      "\n",
      "CA:\n",
      "Whe thale ho\n",
      "BEThiped withalil; s thert by t tr the ar an, th\n",
      "----\n",
      "Ther f ne fo the n:\n",
      "\n",
      "Anoun the hint hand wis r t:\n",
      "And thtit s s s aiste the he the d mowhavar the the lathendy o tha be sthe s ththe s s win the the t it s me t me t the ge t atr an, my ousthend aind t \n",
      "----\n",
      "The thanour t an t he tin the t m thar the n the s s me the the he mand I the wis t the t the t s s the mer man t s wind s the s the the at s s the t ind d the st the be the ne t be s s winou me at the \n",
      "----\n",
      "The the the t the the t the the t the the the the the the the the the the t t the the the the t than the the the the the t the the the the t the the the the the the the t the the the the the the t the t\n"
     ]
    }
   ],
   "source": [
    "print(generate(model,'T', 200, temperature=1))\n",
    "print(\"----\")\n",
    "print(generate(model,'Th', 200, temperature=0.8))\n",
    "print(\"----\")\n",
    "\n",
    "print(generate(model,'Th', 200, temperature=0.5))\n",
    "print(\"----\")\n",
    "\n",
    "print(generate(model,'Th', 200, temperature=0.3))\n",
    "print(\"----\")\n",
    "\n",
    "print(generate(model,'Th', 200, temperature=0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improving this code:\n",
    "\n",
    "(a) Tinker with parameters:\n",
    "\n",
    "- Is it really necessary to have 100 dims character embeddings\n",
    "- Chunk length can be gradually increased\n",
    "- Try changing RNN cell type (GRUs - LSTMs)\n",
    "\n",
    "(b) Add GPU support to go faster\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ------ End of practical\n",
    "\n",
    "#### Legacy loading code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "from os.path import split as pathsplit\n",
    "\n",
    "dir_train = \"aclImdb/train/\"\n",
    "dir_test = \"aclImdb/test/\"\n",
    "\n",
    "train_files = glob.glob(dir_train+'pos/*.txt') + glob.glob(dir_train+'neg/*.txt')\n",
    "test_files = glob.glob(dir_test+'pos/*.txt') + glob.glob(dir_test+'neg/*.txt')\n",
    "\n",
    "\n",
    "def get_polarity(f):\n",
    "    \"\"\"\n",
    "    Extracts polarity from filename:\n",
    "    0 is negative (< 5)\n",
    "    1 is positive (> 5)\n",
    "    \"\"\"\n",
    "    _,name = pathsplit(f)\n",
    "    if int(name.split('_')[1].split('.')[0]) < 5:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "\n",
    "def open_one(f):\n",
    "    \n",
    "    polarity = get_polarity(f)\n",
    "    \n",
    "    with open(f,\"r\") as review:\n",
    "        text = \" \".join(review.readlines()).strip()\n",
    "    \n",
    "    return (text,polarity)\n",
    "\n",
    "print(open_one(train_files[0]))\n",
    "\n",
    "train = [open_one(x) for x in train_files] #contains (text,pol) couples\n",
    "test = [open_one(x) for x in test_files]   #contains (text,pol) couples\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
